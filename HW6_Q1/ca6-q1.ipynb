{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1299795,"sourceType":"datasetVersion","datasetId":751906},{"sourceId":2094438,"sourceType":"datasetVersion","datasetId":1255979}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from IPython.display import clear_output\n!pip install opensimplex\n!pip install noise\nclear_output()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T10:35:41.666666Z","iopub.execute_input":"2025-01-13T10:35:41.667003Z","iopub.status.idle":"2025-01-13T10:35:48.067421Z","shell.execute_reply.started":"2025-01-13T10:35:41.666979Z","shell.execute_reply":"2025-01-13T10:35:48.066446Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom torchvision.utils import make_grid\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport nibabel as nib\nfrom skimage.transform import resize\nfrom tqdm import tqdm\nfrom opensimplex import OpenSimplex\nfrom skimage.filters import threshold_otsu\n\nfrom scipy.ndimage import zoom\nfrom noise import snoise2\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T10:35:48.068769Z","iopub.execute_input":"2025-01-13T10:35:48.069024Z","iopub.status.idle":"2025-01-13T10:35:48.074667Z","shell.execute_reply.started":"2025-01-13T10:35:48.069004Z","shell.execute_reply":"2025-01-13T10:35:48.073652Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\n\nnp.random.seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T10:35:48.076547Z","iopub.execute_input":"2025-01-13T10:35:48.076829Z","iopub.status.idle":"2025-01-13T10:35:48.090450Z","shell.execute_reply.started":"2025-01-13T10:35:48.076780Z","shell.execute_reply":"2025-01-13T10:35:48.089611Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class XIT2Dataset(Dataset):\n    def __init__(self, data_dir, transform=None, target_size=(256, 256), noise_type=None):\n        self.data_dir = data_dir\n        self.transform = transform\n        self.target_size = target_size\n        self.noise_type = noise_type\n        self.image_paths = self._load_image_paths()\n    \n    def _load_image_paths(self):\n        image_paths = []\n        for root, _, files in os.walk(self.data_dir):\n            for file in files:\n                if file.endswith(\".png\"):\n                    image_paths.append(os.path.join(root, file))\n        return sorted(image_paths)\n    \n    def __len__(self):\n        return len(self.image_paths)\n\n    def set_noise_type(self, noise_type):\n        self.noise_type = noise_type\n    \n    def _add_coarse_noise(self, image_tensor):\n        \"\"\"Apply coarse noise.\"\"\"\n        batch_size, height, width = image_tensor.shape\n        coarse_resolution = (16, 16)\n        # Generate coarse Gaussian noise\n        noise = torch.normal(mean=0.0, std=0.2, size=(batch_size, *coarse_resolution), device=image_tensor.device)\n        # Upsample noise to match target size\n        noise = torch.nn.functional.interpolate(noise.unsqueeze(1), size=(height, width), mode='bilinear', align_corners=False)\n        noise = noise.squeeze(1)\n        return torch.clamp(image_tensor + noise, -1, 1)\n\n    def _add_simplex_noise(self, image_tensor):\n        \"\"\"Apply simplex noise with forward diffusion.\"\"\"\n        batch_size, height, width = image_tensor.shape\n        frequency = 2 ** -6\n        octaves = 6\n        persistence = 0.8\n        lacunarity = 2.0\n        diffusion_steps = 87\n\n        # Generate multi-octave noise\n        coords_x = torch.linspace(0, width, steps=width, device=image_tensor.device).repeat(height, 1).T\n        coords_y = torch.linspace(0, height, steps=height, device=image_tensor.device).repeat(width, 1)\n        noise = torch.zeros((batch_size, height, width), device=image_tensor.device)\n\n        for octave in range(octaves):\n            freq = frequency * (lacunarity ** octave)\n            noise += persistence ** octave * torch.sin(freq * (coords_x + coords_y))\n\n        # Normalize noise to [0, 1]\n        noise = (noise - noise.min()) / (noise.max() - noise.min())\n\n        # Forward diffusion process\n        for _ in range(diffusion_steps):\n            image_tensor = torch.clamp(image_tensor + noise * (1 / diffusion_steps), -1, 1)\n\n        return image_tensor\n\n    def _add_noise(self, image_tensor):\n        if self.noise_type == \"coarse\":\n            return self._add_coarse_noise(image_tensor)\n        elif self.noise_type == \"simplex\":\n            return self._add_simplex_noise(image_tensor)\n        return image_tensor\n    \n    def __getitem__(self, idx):\n        image_path = self.image_paths[idx]\n        image = Image.open(image_path).convert(\"L\")  # Load as grayscale\n        image = self.transform(image)\n        if self.noise_type:\n            image = self._add_noise(image)\n        label = torch.tensor(0, dtype=torch.long)  # Healthy label\n        return image, label\n\n    def plot_samples(self, idx, num_samples=8, label_title='Healthy'):\n        if num_samples % 4 != 0:\n            raise ValueError(\"Number of samples must be a multiple of 4.\")\n        fig, axes = plt.subplots(1, num_samples, figsize=(15, 10))\n        for i in range(num_samples):\n            # idx = np.random.randint(len(self.image_paths))\n            image, label = self[idx]\n            print(f\"Image range: {image.min().item()} to {image.max().item()}\")\n            image_np = image.squeeze(0).numpy()\n            axes[i].imshow(image_np, cmap='gray')\n            axes[i].set_title(f\"Label: {label_title}\")\n            axes[i].axis('off')\n        \n        plt.tight_layout()\n        plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T10:35:48.091666Z","iopub.execute_input":"2025-01-13T10:35:48.091949Z","iopub.status.idle":"2025-01-13T10:35:48.106205Z","shell.execute_reply.started":"2025-01-13T10:35:48.091928Z","shell.execute_reply":"2025-01-13T10:35:48.105568Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\nixit1 = XIT2Dataset(\"/kaggle/input/ixit2-slices\", transform)\nidx = np.random.randint(len(ixit1.image_paths))\nixit1.plot_samples(idx, 4, label_title='Healthy')\nixit1.set_noise_type(\"simplex\")\nixit1.plot_samples(idx, 4, label_title='Simplex Noise')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T10:35:48.106998Z","iopub.execute_input":"2025-01-13T10:35:48.107230Z","iopub.status.idle":"2025-01-13T10:36:00.943300Z","shell.execute_reply.started":"2025-01-13T10:35:48.107200Z","shell.execute_reply":"2025-01-13T10:36:00.942504Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ixit2 = XIT2Dataset(\"/kaggle/input/ixit2-slices\", transform)\nixit2.plot_samples(idx, 4, label_title='Healthy')\nixit2.set_noise_type(\"coarse\")\nixit2.plot_samples(idx, 4, label_title='Coarse Noise')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T10:36:00.944218Z","iopub.execute_input":"2025-01-13T10:36:00.944443Z","iopub.status.idle":"2025-01-13T10:36:02.415952Z","shell.execute_reply.started":"2025-01-13T10:36:00.944424Z","shell.execute_reply":"2025-01-13T10:36:02.414969Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class VAE(nn.Module):\n    def __init__(self, input_channels=1, latent_dim=128):\n        super(VAE, self).__init__()\n        self.latent_dim = latent_dim\n\n        self.encoder = nn.Sequential(\n            nn.Conv2d(input_channels, 32, kernel_size=4, stride=2, padding=1),  # 128x128\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),              # 64x64\n            nn.ReLU(),\n            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),             # 32x32\n            nn.ReLU(),\n            nn.Flatten()\n        )\n        self.fc_mu = nn.Linear(128 * 32 * 32, latent_dim)  # Latent space mean\n        self.fc_logvar = nn.Linear(128 * 32 * 32, latent_dim)  # Latent space log variance\n\n        self.fc_decoder = nn.Linear(latent_dim, 128 * 32 * 32)\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),   # 64x64\n            nn.ReLU(),\n            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),    # 128x128\n            nn.ReLU(),\n            nn.ConvTranspose2d(32, input_channels, kernel_size=4, stride=2, padding=1),  # 256x256\n            nn.Sigmoid()\n        )\n\n    def reparameterize(self, mu, logvar):\n        \"\"\"Reparameterization trick for sampling.\"\"\"\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def forward(self, x):\n        # Encode\n        z = self.encoder(x)\n        mu = self.fc_mu(z)\n        logvar = self.fc_logvar(z)\n        # Sample latent vector\n        z_sampled = self.reparameterize(mu, logvar)\n        # Decode\n        z_decoded = self.fc_decoder(z_sampled)\n        z_decoded = z_decoded.view(-1, 128, 32, 32)\n        reconstruction = self.decoder(z_decoded)\n        return reconstruction, mu, logvar\n\n# Loss function\ndef vae_loss_function(reconstruction, input, mu, logvar):\n    # Reconstruction loss\n    reconstruction_loss = nn.MSELoss()(reconstruction, input)\n    # KL Divergence\n    kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n    return reconstruction_loss + kl_divergence / input.size(0), reconstruction_loss, kl_divergence\n\ndef train_vae(vae, train_loader, optimizer, num_epochs=20):\n    vae.train()\n    train_losses = []\n    for epoch in range(num_epochs):\n        epoch_loss = 0.0\n        with tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\") as tepoch:\n            for batch_idx, (images, _) in enumerate(train_loader):\n                images = images.to(device)\n                optimizer.zero_grad()\n                reconstruction, mu, logvar = vae(images)\n                loss, recon_loss, kl_loss = vae_loss_function(reconstruction, images, mu, logvar)\n                loss.backward()\n                optimizer.step()\n                epoch_loss += loss.item()\n                tepoch.set_postfix(loss=loss.item())\n                tepoch.update(1)\n        epoch_loss /= len(train_loader)\n        train_losses.append(epoch_loss)\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n    return train_losses\n\ndef plot_loss(train_losses):\n    plt.figure(figsize=(10, 5))\n    plt.plot(train_losses, label='Training Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.title('Training Loss over Epochs')\n    plt.legend()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T10:36:02.416958Z","iopub.execute_input":"2025-01-13T10:36:02.417248Z","iopub.status.idle":"2025-01-13T10:36:02.432309Z","shell.execute_reply.started":"2025-01-13T10:36:02.417222Z","shell.execute_reply":"2025-01-13T10:36:02.431378Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Hyperparameters\nbatch_size = 64\nlatent_dim = 128\nlearning_rate = 1e-4\nnum_epochs = 20\n\nixit1.set_noise_type(None)\ntrain_loader = DataLoader(ixit1, batch_size=batch_size, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T10:36:02.434678Z","iopub.execute_input":"2025-01-13T10:36:02.434918Z","iopub.status.idle":"2025-01-13T10:36:02.452972Z","shell.execute_reply.started":"2025-01-13T10:36:02.434898Z","shell.execute_reply":"2025-01-13T10:36:02.452278Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vae = VAE(input_channels=1, latent_dim=latent_dim).to(device)\noptimizer = optim.Adam(vae.parameters(), lr=learning_rate)\n\ntrain_losses = train_vae(vae, train_loader, optimizer, 3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T10:40:09.442575Z","iopub.execute_input":"2025-01-13T10:40:09.442891Z","iopub.status.idle":"2025-01-13T10:47:38.819209Z","shell.execute_reply.started":"2025-01-13T10:40:09.442867Z","shell.execute_reply":"2025-01-13T10:47:38.818373Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_loss(train_losses)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T10:47:38.820276Z","iopub.execute_input":"2025-01-13T10:47:38.820567Z","iopub.status.idle":"2025-01-13T10:47:39.092645Z","shell.execute_reply.started":"2025-01-13T10:47:38.820546Z","shell.execute_reply":"2025-01-13T10:47:39.091668Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class BraTSDataset(Dataset):\n    def __init__(self, data_dir, transform=None, target_size=(256, 256)):\n        self.data_dir = data_dir\n        self.transform = transform\n        self.target_size = target_size\n        self.image_paths, self.mask_paths = self._load_paths()\n    \n    def _load_paths(self):\n        image_paths = []\n        mask_paths = []\n        for root, _, files in os.walk(self.data_dir):\n            for file in files:\n                if file.endswith(\"_t2.nii\"):\n                    image_paths.append(os.path.join(root, file))\n                    mask_paths.append(os.path.join(root, file.replace(\"_t2.nii\", \"_seg.nii\")))\n        return sorted(image_paths), sorted(mask_paths)\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        image_path = self.image_paths[idx]\n        mask_path = self.mask_paths[idx]\n        \n        image = nib.load(image_path).get_fdata()\n        mask = nib.load(mask_path).get_fdata()\n        \n        # Extract the middle slice along the z-axis\n        z_index = image.shape[-1] // 2\n        image = image[:, :, z_index]  # Extract 2D slice\n        mask = mask[:, :, z_index]   # Extract 2D slice\n    \n        image = (image - np.min(image)) / (np.max(image) - np.min(image))\n        image = resize(image, self.target_size, order=1, mode='reflect', anti_aliasing=True)\n        mask = resize(mask, self.target_size, order=0, mode='reflect', anti_aliasing=False)\n    \n        # Convert NumPy arrays back to PIL images for transformation\n        image = Image.fromarray((image * 255).astype(np.uint8))\n        mask = Image.fromarray((mask * 255).astype(np.uint8))\n    \n        if self.transform:\n            image = self.transform(image)\n            mask = self.transform(mask)\n        else: # Convert to tensors\n            image = torch.tensor(np.array(image), dtype=torch.float32).unsqueeze(0)  # Add channel dim\n            mask = torch.tensor(np.array(mask), dtype=torch.float32)\n    \n        return image, mask\n\n    def plot_samples(self, num_samples=4):\n        if num_samples % 2 != 0:\n            raise ValueError(\"Number of samples must be even.\")        \n        fig, axes = plt.subplots(2, num_samples, figsize=(15, 5))\n        for i in range(num_samples):\n            idx = np.random.randint(len(self.image_paths))\n            image, mask = self[idx]\n            image_np = image.squeeze(0).numpy() if isinstance(image, torch.Tensor) else image\n            mask_np = mask.squeeze(0).numpy() if len(mask.shape) == 3 else mask\n            axes[0, i].imshow(image_np, cmap='gray')\n            axes[0, i].set_title(\"T2 Image\")\n            axes[0, i].axis('off')\n            axes[1, i].imshow(mask_np, cmap='gray')\n            axes[1, i].set_title(\"Segmentation Mask\")\n            axes[1, i].axis('off')\n        plt.tight_layout()\n        plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T10:47:39.094617Z","iopub.execute_input":"2025-01-13T10:47:39.094981Z","iopub.status.idle":"2025-01-13T10:47:39.107879Z","shell.execute_reply.started":"2025-01-13T10:47:39.094953Z","shell.execute_reply":"2025-01-13T10:47:39.107148Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def dice_coefficient(predicted_mask, ground_truth_mask):\n    smooth = 1e-6  # To avoid division by zero\n    predicted_mask = predicted_mask.flatten()\n    ground_truth_mask = ground_truth_mask.flatten()\n    intersection = (predicted_mask * ground_truth_mask).sum()\n    return (2. * intersection + smooth) / (predicted_mask.sum() + ground_truth_mask.sum() + smooth)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T10:47:39.108726Z","iopub.execute_input":"2025-01-13T10:47:39.109032Z","iopub.status.idle":"2025-01-13T10:47:39.125204Z","shell.execute_reply.started":"2025-01-13T10:47:39.108997Z","shell.execute_reply":"2025-01-13T10:47:39.124418Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_vae(vae, dataset, threshold=0.5, num_samples=4):\n    vae.eval()\n    dice_scores = []\n    \n    # Randomly sample indices\n    random_indices = random.sample(range(len(dataset)), num_samples)\n    \n    for idx in random_indices:\n        image, ground_truth_mask = dataset[idx]\n        image = image.to(device).unsqueeze(0)  # Add batch dimension\n\n        # VAE reconstruction\n        with torch.no_grad():\n            reconstruction, _, _ = vae(image)\n        \n        # Residual calculation\n        residual = torch.abs(image - reconstruction)  # Shape: (1, 1, 256, 256)\n        \n        # Predicted mask from residual\n        predicted_mask = (residual > threshold).float()  # Binary mask\n        \n        # Dice coefficient\n        dice = dice_coefficient(\n            predicted_mask.squeeze().cpu().numpy(),  # Convert to 2D NumPy\n            ground_truth_mask.numpy()\n        )\n        dice_scores.append(dice)\n\n        # Visualization for each random sample\n        plt.figure(figsize=(20, 5))\n        \n        # Original Image\n        plt.subplot(1, 5, 1)\n        plt.imshow(image.squeeze().cpu().numpy(), cmap='gray')  # Shape: (256, 256)\n        plt.title(\"Original Image\")\n        plt.axis('off')\n        \n        # Reconstructed Image\n        plt.subplot(1, 5, 2)\n        plt.imshow(reconstruction.squeeze().cpu().numpy(), cmap='gray')  # Shape: (256, 256)\n        plt.title(\"Reconstructed Image\")\n        plt.axis('off')\n        \n        # Residual Image\n        plt.subplot(1, 5, 3)\n        plt.imshow(residual.squeeze().cpu().numpy(), cmap='gray')  # Shape: (256, 256)\n        plt.title(\"Residual Image\")\n        plt.axis('off')\n        \n        # Predicted Mask\n        plt.subplot(1, 5, 4)\n        plt.imshow(predicted_mask.squeeze().cpu().numpy(), cmap='gray')  # Shape: (256, 256)\n        plt.title(\"Predicted Mask\")\n        plt.axis('off')\n        \n        # Ground Truth Mask\n        plt.subplot(1, 5, 5)\n        plt.imshow(ground_truth_mask.squeeze().cpu().numpy(), cmap='gray')  # Shape: (256, 256)\n        plt.title(\"Ground Truth Mask\")\n        plt.axis('off')\n        \n        plt.tight_layout()\n        plt.show()\n\n    # Overall Dice Score\n    avg_dice_score = sum(dice_scores) / len(dice_scores)\n    print(f\"Average Dice Score: {avg_dice_score:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T10:54:00.804065Z","iopub.execute_input":"2025-01-13T10:54:00.804372Z","iopub.status.idle":"2025-01-13T10:54:00.812498Z","shell.execute_reply.started":"2025-01-13T10:54:00.804350Z","shell.execute_reply":"2025-01-13T10:54:00.811575Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assuming the dataset and trained VAE are loaded\ndata_dir = \"/kaggle/input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData\"\nbrats_dataset = BraTSDataset(data_dir=data_dir, transform=transform)\n\n# Evaluate VAE with random sample plotting\nevaluate_vae(vae, brats_dataset, threshold=0.5, num_samples=10)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T10:54:04.003659Z","iopub.execute_input":"2025-01-13T10:54:04.003990Z","iopub.status.idle":"2025-01-13T10:54:12.729588Z","shell.execute_reply.started":"2025-01-13T10:54:04.003963Z","shell.execute_reply":"2025-01-13T10:54:12.728689Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# transform = transforms.Compose([\n#     transforms.ToTensor(),\n#     transforms.Normalize((0.5,), (0.5,))\n# ])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T10:40:04.649182Z","iopub.status.idle":"2025-01-13T10:40:04.649554Z","shell.execute_reply":"2025-01-13T10:40:04.649390Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ixit = XIT2Dataset(\"/kaggle/input/ixit2-slices\", transform)\n# ixit.plot_samples(4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T10:40:04.650367Z","iopub.status.idle":"2025-01-13T10:40:04.650727Z","shell.execute_reply":"2025-01-13T10:40:04.650569Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# data_dir = \"/kaggle/input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData\"\n# dataset = BraTSDataset(data_dir=data_dir, transform=transform)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T10:40:04.651495Z","iopub.status.idle":"2025-01-13T10:40:04.651884Z","shell.execute_reply":"2025-01-13T10:40:04.651704Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset.plot_samples()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T10:40:04.652588Z","iopub.status.idle":"2025-01-13T10:40:04.653007Z","shell.execute_reply":"2025-01-13T10:40:04.652843Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# class Encoder(nn.Module):\n#     def __init__(self, latent_dim):\n#         super(Encoder, self).__init__()\n#         self.conv = nn.Sequential(\n#             nn.Conv2d(1, 32, 4, 2, 1), nn.ReLU(),\n#             nn.Conv2d(32, 64, 4, 2, 1), nn.ReLU(),\n#             nn.Conv2d(64, 128, 4, 2, 1), nn.ReLU(),\n#             nn.Conv2d(128, 256, 4, 2, 1), nn.ReLU()\n#         )\n#         self.fc_mu = nn.Linear(256 * 16 * 16, latent_dim)\n#         self.fc_logvar = nn.Linear(256 * 16 * 16, latent_dim)\n\n#     def forward(self, x):\n#         x = self.conv(x)\n#         x = x.view(x.size(0), -1)\n#         mu = self.fc_mu(x)\n#         logvar = self.fc_logvar(x)\n#         return mu, logvar\n\n# class Decoder(nn.Module):\n#     def __init__(self, latent_dim):\n#         super(Decoder, self).__init__()\n#         self.fc = nn.Linear(latent_dim, 256 * 16 * 16)\n#         self.deconv = nn.Sequential(\n#             nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.ReLU(),\n#             nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.ReLU(),\n#             nn.ConvTranspose2d(64, 32, 4, 2, 1), nn.ReLU(),\n#             nn.ConvTranspose2d(32, 1, 4, 2, 1), nn.Tanh()\n#         )\n\n#     def forward(self, z):\n#         x = self.fc(z)\n#         x = x.view(x.size(0), 256, 16, 16)\n#         x = self.deconv(x)\n#         return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T10:40:04.653916Z","iopub.status.idle":"2025-01-13T10:40:04.654216Z","shell.execute_reply":"2025-01-13T10:40:04.654092Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# class TriVAE(nn.Module):\n#     def __init__(self, latent_dim):\n#         super(TriVAE, self).__init__()\n#         self.encoder = Encoder(latent_dim)\n#         self.decoder = Decoder(latent_dim)\n\n#     def reparameterize(self, mu, logvar):\n#         std = torch.exp(0.5 * logvar)\n#         eps = torch.randn_like(std)\n#         return mu + eps * std\n\n#     def forward(self, x):\n#         mu, logvar = self.encoder(x)\n#         z = self.reparameterize(mu, logvar)\n#         recon_x = self.decoder(z)\n#         return recon_x, mu, logvar","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T10:40:04.655352Z","iopub.status.idle":"2025-01-13T10:40:04.655652Z","shell.execute_reply":"2025-01-13T10:40:04.655548Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def vae_loss(recon_x, x, mu, logvar):\n#     recon_loss = nn.L1Loss()(recon_x, x)\n#     kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n#     return recon_loss + kld_loss\n\n# def triplet_loss(anchor, positive, negative, margin=1.0):\n#     dist_pos = torch.norm(anchor - positive, p=2, dim=1)\n#     dist_neg = torch.norm(anchor - negative, p=2, dim=1)\n#     return torch.mean(torch.relu(dist_pos - dist_neg + margin))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T10:40:04.656421Z","iopub.status.idle":"2025-01-13T10:40:04.656732Z","shell.execute_reply":"2025-01-13T10:40:04.656595Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def apply_noise(noise_type, images):\n#     if noise_type == 'coarse':\n#         # Generate coarse noise directly in PyTorch\n#         noise = torch.randn((images.size(0), 1, 32, 32), device=device) * 0.2  # Low-res Gaussian noise\n#         noise = torch.nn.functional.interpolate(noise, size=images.shape[2:], mode='bilinear', align_corners=False)\n#         return images + noise\n#     elif noise_type == 'simplex':\n#         # Generate simplex noise directly in PyTorch\n#         simplex = OpenSimplex(seed=42)\n#         scale = 1.0 / 64.0\n#         noise = torch.zeros_like(images, device=device)\n#         for i in range(images.size(2)):  # Height\n#             for j in range(images.size(3)):  # Width\n#                 noise[:, :, i, j] = torch.tensor([simplex.noise2(i * scale, j * scale) for _ in range(images.size(0))], device=device)\n#         return images + noise\n#     else:\n#         return images.clone()  # No noise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T10:40:04.657629Z","iopub.status.idle":"2025-01-13T10:40:04.658002Z","shell.execute_reply":"2025-01-13T10:40:04.657889Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def train(model, dataloader, optimizer, dataset, noise_type='coarse', epochs=10):\n#     model.to(device)\n#     model.train()\n    \n#     for epoch in range(epochs):\n#         total_loss = 0\n#         print(f\"Epoch {epoch + 1}/{epochs}\")\n#         for images, _ in tqdm(dataloader, desc=f\"Training Epoch {epoch + 1}\"):\n#             images = images.to(device)\n#             # Generate anchor, positive, and negative samples\n#             anchor = images\n#             positive = images.clone()\n#             # Add noise to negative samples explicitly\n#             negative = apply_noise(noise_type, images) \n#             optimizer.zero_grad()\n#             # Forward pass for each sample\n#             recon_anchor, mu_anchor, logvar_anchor = model(anchor)\n#             recon_positive, mu_positive, logvar_positive = model(positive)\n#             recon_negative, mu_negative, logvar_negative = model(negative)\n#             # Compute losses\n#             vae_loss_anchor = vae_loss(recon_anchor, anchor, mu_anchor, logvar_anchor)\n#             vae_loss_positive = vae_loss(recon_positive, positive, mu_positive, logvar_positive)\n#             vae_loss_negative = vae_loss(recon_negative, negative, mu_negative, logvar_negative)\n#             t_loss = triplet_loss(mu_anchor, mu_positive, mu_negative)\n#             # Combine losses\n#             loss = vae_loss_anchor + vae_loss_positive + vae_loss_negative + t_loss\n#             loss.backward()\n#             optimizer.step()\n#             total_loss += loss.item()\n        \n#         print(f\"Epoch [{epoch + 1}/{epochs}] - Loss: {total_loss / len(dataloader):.4f}\")\n#     torch.save(model.state_dict(), 'model_final.pth') \n#     print(\"Model saved!\")\n\n# def evaluate(model, dataloader):\n#     model.eval()\n#     residual_maps = []\n#     with torch.no_grad():\n#         for images, _ in tqdm(dataloader, desc=\"Evaluating\"):\n#             images = images.to(device)\n#             recon_images, _, _ = model(images)\n#             residual_map = torch.abs(images - recon_images).cpu().numpy()\n#             residual_maps.append(residual_map)\n#     return np.concatenate(residual_maps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T10:40:04.658597Z","iopub.status.idle":"2025-01-13T10:40:04.658910Z","shell.execute_reply":"2025-01-13T10:40:04.658750Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_loader = DataLoader(ixit, batch_size=128, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T10:40:04.659818Z","iopub.status.idle":"2025-01-13T10:40:04.660184Z","shell.execute_reply":"2025-01-13T10:40:04.660006Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# latent_dim = 512\n# model = TriVAE(latent_dim)\n# optimizer = optim.Adam(model.parameters(), lr=1e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T10:40:04.661311Z","iopub.status.idle":"2025-01-13T10:40:04.661632Z","shell.execute_reply":"2025-01-13T10:40:04.661475Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train(model, train_loader, optimizer, ixit, noise_type='coarse', epochs=20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T10:40:04.662529Z","iopub.status.idle":"2025-01-13T10:40:04.662863Z","shell.execute_reply":"2025-01-13T10:40:04.662693Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# import numpy as np\n# import matplotlib.pyplot as plt\n# from skimage.filters import threshold_otsu\n\n# def evaluate_and_visualize(model, dataset, device, num_samples=4):\n#     \"\"\"\n#     Evaluate the model on BraTS slices, compute Dice score, and visualize results.\n    \n#     Args:\n#         model (nn.Module): Trained Tri-VAE model.\n#         dataset (Dataset): BraTSDataset instance.\n#         device (torch.device): Device to run the model on (CPU or GPU).\n#         num_samples (int): Number of slices to evaluate and visualize.\n#     \"\"\"\n#     model.to(device)\n#     model.eval()\n\n#     fig, axes = plt.subplots(num_samples, 4, figsize=(20, 5 * num_samples))\n#     dice_scores = []\n\n#     with torch.no_grad():\n#         for i in range(num_samples):\n#             # Randomly select a tumor slice\n#             idx = np.random.randint(len(dataset))\n#             image, mask = dataset[idx]\n#             image = image.to(device).unsqueeze(0)  # Add batch dimension\n\n#             # Forward pass through the model\n#             recon_image, _, _ = model(image)\n#             residual_map = torch.abs(image - recon_image).squeeze(0).cpu().numpy()\n\n#             # Threshold residual map to identify tumor region\n#             threshold = threshold_otsu(residual_map)\n#             predicted_mask = (residual_map > threshold).astype(np.uint8)\n\n#             # Compute Dice score\n#             ground_truth = mask.numpy().astype(np.uint8)\n#             intersection = np.sum(predicted_mask * ground_truth)\n#             dice = (2.0 * intersection) / (np.sum(predicted_mask) + np.sum(ground_truth) + 1e-6)\n#             dice_scores.append(dice)\n\n#             # Visualize results\n#             axes[i, 0].imshow(image.squeeze(0).cpu().numpy(), cmap='gray')\n#             axes[i, 0].set_title(\"Input Slice\")\n#             axes[i, 0].axis('off')\n\n#             axes[i, 1].imshow(recon_image.squeeze(0).cpu().numpy(), cmap='gray')\n#             axes[i, 1].set_title(\"Reconstructed Slice\")\n#             axes[i, 1].axis('off')\n\n#             axes[i, 2].imshow(residual_map, cmap='hot')\n#             axes[i, 2].set_title(\"Residual Map\")\n#             axes[i, 2].axis('off')\n\n#             axes[i, 3].imshow(ground_truth, cmap='gray', alpha=0.5, label='Ground Truth')\n#             axes[i, 3].imshow(predicted_mask, cmap='jet', alpha=0.5, label='Predicted Mask')\n#             axes[i, 3].set_title(f\"Tumor Prediction (Dice: {dice:.4f})\")\n#             axes[i, 3].axis('off')\n\n#     plt.tight_layout()\n#     plt.show()\n\n#     print(f\"Average Dice Score: {np.mean(dice_scores):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T10:40:04.665421Z","iopub.status.idle":"2025-01-13T10:40:04.665754Z","shell.execute_reply":"2025-01-13T10:40:04.665593Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# brats_dataset = BraTSDataset(\n#     data_dir=data_dir,\n#     transform=transforms.Compose([transforms.ToTensor()]),\n#     target_size=(256, 256)\n# )\n\n# # Evaluate and visualize\n# evaluate_and_visualize(model, brats_dataset, device, num_samples=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T10:40:04.666578Z","iopub.status.idle":"2025-01-13T10:40:04.666901Z","shell.execute_reply":"2025-01-13T10:40:04.666750Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def evaluate_brats(model, dataset, num_patients=100, threshold=0.1, num_samples_to_display=5):\n#     \"\"\" Evaluate the trained Tri-VAE model on BraTS tumor slices and compute Dice scores \"\"\"\n#     model.to(device)\n#     model.eval()\n    \n#     dice_scores = []\n#     samples_to_display = []    \n#     with torch.no_grad():\n#         for idx in range(min(num_patients, len(dataset))):\n#             image, mask = dataset[idx]\n#             image = image.to(device).unsqueeze(0)  # Add batch dimension\n#             # Forward pass through the model\n#             recon_image, _, _ = model(image)\n#             residual_map = torch.abs(image - recon_image).squeeze(0).cpu().numpy()            \n#             # Threshold residual map to identify tumor region\n#             predicted_mask = (residual_map > threshold).astype(np.uint8)\n            \n#             ground_truth = mask.numpy().astype(np.uint8)\n#             intersection = np.sum(predicted_mask * ground_truth)\n#             dice = (2.0 * intersection) / (np.sum(predicted_mask) + np.sum(ground_truth) + 1e-6)\n#             dice_scores.append(dice)\n#             if len(samples_to_display) < num_samples_to_display:\n#                 samples_to_display.append((image.squeeze(0).cpu().numpy(), residual_map, predicted_mask, ground_truth))\n    \n#     visualize_results(samples_to_display, dice_scores[:num_samples_to_display])\n#     return np.mean(dice_scores)\n\n# def visualize_results(samples, dice_scores):\n#     \"\"\" Visualize input slices, residual maps, predicted masks, and ground truth masks \"\"\"\n#     num_samples = len(samples)\n#     fig, axes = plt.subplots(num_samples, 4, figsize=(20, 5 * num_samples))\n#     for i, (input_slice, residual_map, predicted_mask, ground_truth) in enumerate(samples):\n#         input_slice = input_slice.squeeze()\n#         residual_map = residual_map.squeeze()\n#         predicted_mask = predicted_mask.squeeze()\n#         ground_truth = ground_truth.squeeze()\n#         axes[i, 0].imshow(input_slice, cmap='gray')\n#         axes[i, 0].set_title(\"Input Slice\")\n#         axes[i, 0].axis('off')\n#         axes[i, 1].imshow(residual_map, cmap='hot')\n#         axes[i, 1].set_title(\"Residual Map\")\n#         axes[i, 1].axis('off')\n#         axes[i, 2].imshow(predicted_mask, cmap='gray', alpha=1)\n#         axes[i, 2].set_title(f\"Predicted Mask (Dice: {dice_scores[i]:.4f})\")\n#         axes[i, 2].axis('off')\n#         axes[i, 3].imshow(ground_truth, cmap='gray', alpha=1)\n#         axes[i, 3].set_title(\"Ground Truth Mask\")\n#         axes[i, 3].axis('off')\n#     plt.tight_layout()\n#     plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T10:40:04.667924Z","iopub.status.idle":"2025-01-13T10:40:04.668285Z","shell.execute_reply":"2025-01-13T10:40:04.668160Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# brats_dataset = BraTSDataset(\n#     data_dir=data_dir,\n#     transform=transforms.Compose([transforms.ToTensor()]),\n#     target_size=(256, 256)\n# )\n# average_dice = evaluate_brats(model, brats_dataset, num_patients=100, threshold=0.1, num_samples_to_display=10)\n# print(f\"Average Dice Score: {average_dice:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T10:40:04.669084Z","iopub.status.idle":"2025-01-13T10:40:04.669407Z","shell.execute_reply":"2025-01-13T10:40:04.669265Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T11:29:35.818866Z","iopub.execute_input":"2025-01-13T11:29:35.819240Z","iopub.status.idle":"2025-01-13T11:29:35.823404Z","shell.execute_reply.started":"2025-01-13T11:29:35.819212Z","shell.execute_reply":"2025-01-13T11:29:35.822431Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"class ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n        super(ConvBlock, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n        self.norm = nn.GroupNorm(8, out_channels)\n        self.activation = nn.SiLU()\n\n    def forward(self, x):\n        return self.activation(self.norm(self.conv(x)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T11:38:06.931041Z","iopub.execute_input":"2025-01-13T11:38:06.931376Z","iopub.status.idle":"2025-01-13T11:38:06.936361Z","shell.execute_reply.started":"2025-01-13T11:38:06.931348Z","shell.execute_reply":"2025-01-13T11:38:06.935320Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"class DecoderBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n        self.norm = nn.GroupNorm(num_groups=8, num_channels=out_channels)\n        self.activation = Swish()\n\n    def forward(self, x):\n        return self.activation(self.norm(self.conv(x)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T11:34:34.140784Z","iopub.execute_input":"2025-01-13T11:34:34.141131Z","iopub.status.idle":"2025-01-13T11:34:34.145953Z","shell.execute_reply.started":"2025-01-13T11:34:34.141108Z","shell.execute_reply":"2025-01-13T11:34:34.145022Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"class VAE(nn.Module):\n    def __init__(self):\n        super(VAE, self).__init__()\n\n        # Encoder\n        self.encoder_blocks = nn.ModuleList([\n            ConvBlock(1, 16),\n            ConvBlock(16, 32),\n            ConvBlock(32, 64),\n            ConvBlock(64, 128),\n            ConvBlock(128, 256),\n            ConvBlock(256, 512),\n        ])\n        self.avgpool = nn.AvgPool2d(2)\n\n        # Bottleneck\n        self.fc_mu = nn.Linear(512 * 8 * 8, 1024)\n        self.fc_logvar = nn.Linear(512 * 8 * 8, 1024)\n        self.fc_decode = nn.Linear(1024, 512 * 8 * 8)\n\n        # Decoder\n        self.upconvs = nn.ModuleList([\n            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),\n            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n            nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2),\n            nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2),\n        ])\n        self.decoder_blocks = nn.ModuleList([\n            ConvBlock(512, 256),\n            ConvBlock(256, 128),\n            ConvBlock(128, 64),\n            ConvBlock(64, 32),\n            ConvBlock(32, 16),\n        ])\n\n        self.final_conv = nn.Conv2d(16, 1, kernel_size=1)\n\n    def reparameterize(self, mu, logvar):\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def forward(self, x):\n        # Encoder\n        skips = []\n        for block in self.encoder_blocks:\n            x = block(x)\n            skips.append(x)\n            x = self.avgpool(x)\n\n        # Bottleneck\n        x = x.view(x.size(0), -1)\n        mu = self.fc_mu(x)\n        logvar = self.fc_logvar(x)\n        z = self.reparameterize(mu, logvar)\n        x = self.fc_decode(z)\n        x = x.view(x.size(0), 512, 8, 8)\n\n        # Decoder\n        for i, block in enumerate(self.decoder_blocks):\n            x = self.upconvs[i](x)\n            skip = skips[-(i+1)]\n\n            # Align spatial dimensions of x and skip\n            if x.size(2) != skip.size(2) or x.size(3) != skip.size(3):\n                x = F.interpolate(x, size=(skip.size(2), skip.size(3)), mode=\"nearest\")\n\n            x = torch.cat([x, skip], dim=1)\n            x = block(x)\n\n        # Final layer\n        return self.final_conv(x), mu, logvar","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T11:38:09.561422Z","iopub.execute_input":"2025-01-13T11:38:09.561749Z","iopub.status.idle":"2025-01-13T11:38:09.571364Z","shell.execute_reply.started":"2025-01-13T11:38:09.561722Z","shell.execute_reply":"2025-01-13T11:38:09.570335Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"vae = VAE().to(device)\noptimizer = optim.Adam(vae.parameters(), lr=learning_rate)\n\ntrain_losses = train_vae(vae, train_loader, optimizer, 3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T11:39:28.570682Z","iopub.execute_input":"2025-01-13T11:39:28.571065Z","iopub.status.idle":"2025-01-13T11:39:29.799517Z","shell.execute_reply.started":"2025-01-13T11:39:28.571033Z","shell.execute_reply":"2025-01-13T11:39:29.798309Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/3:   0%|          | 0/451 [00:00<?, ?batch/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-67-df0587891b98>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_vae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-34-daedc3b3406e>\u001b[0m in \u001b[0;36mtrain_vae\u001b[0;34m(vae, train_loader, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0mreconstruction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecon_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae_loss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreconstruction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-65-8c9a929eb144>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# Bottleneck\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_mu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_logvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreparameterize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x8192 and 32768x1024)"],"ename":"RuntimeError","evalue":"mat1 and mat2 shapes cannot be multiplied (64x8192 and 32768x1024)","output_type":"error"}],"execution_count":67},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}