{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport re\nimport cv2\nimport json\nimport random\nimport numpy as np\nfrom tqdm import tqdm\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom collections import defaultdict\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.models as models\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.models.efficientnet import EfficientNet_B0_Weights\nfrom torchvision.models import efficientnet_b0\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom nltk.translate.bleu_score import sentence_bleu","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T23:19:37.027134Z","iopub.execute_input":"2025-01-26T23:19:37.027724Z","iopub.status.idle":"2025-01-26T23:19:44.435491Z","shell.execute_reply.started":"2025-01-26T23:19:37.027680Z","shell.execute_reply":"2025-01-26T23:19:44.433702Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# //TODO::WTF::\n\nIMAGES_DIR = \"/kaggle/input/flickr8k/Images\"\nCAPTIONS_FILE = \"/kaggle/input/flickr8k/captions.txt\"\n\nBATCH_SIZE = 64\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T23:19:44.437286Z","iopub.execute_input":"2025-01-26T23:19:44.437916Z","iopub.status.idle":"2025-01-26T23:19:44.450804Z","shell.execute_reply.started":"2025-01-26T23:19:44.437873Z","shell.execute_reply":"2025-01-26T23:19:44.449649Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"device(type='cpu')"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"def load_captions(captions_file):\n    captions_dict = {}\n    with open(captions_file, 'r') as file:\n        for line in file:\n            tokens = line.strip().split(\",\") \n            if len(tokens) == 2:\n                image_id, caption = tokens\n                if image_id not in captions_dict:\n                    captions_dict[image_id] = []\n                captions_dict[image_id].append(caption)\n    return captions_dict\n\ndef display_images_with_captions(images_dir, captions_dict, num_images=3, image_size=(224, 224)):\n    images = list(captions_dict.keys())\n\n    if len(images) < num_images:\n        raise ValueError(f\"Requested {num_images} images, but only {len(images)} available.\")\n    \n    selected_images = random.sample(images, num_images)\n    plt.figure(figsize=(10 * num_images, 10))\n    \n    for i, image in enumerate(selected_images):\n        img_path = os.path.join(images_dir, image)\n        img = Image.open(img_path)\n        \n        plt.subplot(num_images, 1, i + 1)\n        plt.imshow(img)\n        plt.axis('off')\n        \n        captions = \"\\n\".join(captions_dict[image]) \n        plt.title(captions, fontsize=10, loc='center', wrap=True)\n    \n    plt.tight_layout()\n    plt.show()\n\ndef plot_training_losses(train_losses, val_losses):\n    plt.figure(figsize=(10, 6))\n    plt.plot(range(1, len(train_losses) + 1), train_losses, marker='o', label='Training Loss')\n    plt.plot(range(1, len(val_losses) + 1), val_losses, marker='x', label='Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.title('Training and Validation Loss Over Epochs')\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n\ndef visualize_results(model, dataloader, tokenizer, num_images=5):\n    for images, _ in dataloader:\n        images = images[:num_images].to(device)\n        captions = [generate_caption(model, image, tokenizer) for image in images]\n\n        plt.figure(figsize=(5, num_images * 5))\n\n        for i in range(num_images):\n            ax = plt.subplot(num_images, 1, i + 1)\n            ax.imshow(images[i].permute(1, 2, 0).cpu().numpy())\n            ax.axis(\"off\")  # Remove axes\n            ax.set_title(captions[i], fontsize=12, wrap=True)\n\n        plt.tight_layout()\n        plt.show()\n        break\n\n# def display_random_images_with_captions(images_dir, captions_dict, num_images=5):\n#     selected_images = random.sample(list(captions_dict.keys()), num_images)\n#     plt.figure(figsize=(15, 10))\n\n#     for i, image_id in enumerate(selected_images):\n#         img_path = f\"{images_dir}/{image_id}\"\n#         img = Image.open(img_path)\n\n#         plt.subplot(num_images, 1, i + 1)\n#         # plt.subplot(1, num_images, i + 1)\n#         plt.imshow(img)\n#         plt.axis(\"off\")\n#         plt.title(captions_dict[image_id], fontsize=10)\n    \n#     plt.tight_layout()\n#     plt.show()\n        \ndef plot_caption_length_variation(captions_dict):\n    lengths = []\n    for captions in captions_dict.values():\n        lengths.extend([len(caption.split()) for caption in captions])\n    \n    plt.figure(figsize=(15, 10))\n    plt.scatter(range(len(lengths)), lengths, alpha=0.5)\n    plt.title(\"Variation in Caption Lengths\", fontsize=14)\n    plt.xlabel(\"Caption Index\", fontsize=12)\n    plt.ylabel(\"Caption Length (Number of Words)\", fontsize=12)\n    plt.grid(True, alpha=0.3)\n    plt.show()\n\ndef plot_word_frequency_histogram(captions_dict, top_n=20):\n    word_counter = Counter()\n    for captions in captions_dict.values():\n        for caption in captions:\n            preprocessed_caption = preprocess_text(caption)\n            word_counter.update(preprocessed_caption.split())\n    \n    most_common_words = word_counter.most_common(top_n)\n    words, counts = zip(*most_common_words)\n\n    plt.figure(figsize=(12, 6))\n    plt.bar(words, counts, color='skyblue')\n    plt.title(f\"Top {top_n} Most Frequent Words\", fontsize=14)\n    plt.xlabel(\"Words\", fontsize=12)\n    plt.ylabel(\"Frequency\", fontsize=12)\n    plt.xticks(rotation=45, fontsize=10)\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T23:19:44.453013Z","iopub.execute_input":"2025-01-26T23:19:44.453301Z","iopub.status.idle":"2025-01-26T23:19:44.474386Z","shell.execute_reply.started":"2025-01-26T23:19:44.453278Z","shell.execute_reply":"2025-01-26T23:19:44.473246Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def split_dataset(captions_dict, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, random_seed=42):\n    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \"Ratios must sum to 1.\"\n\n    flattened_captions = []\n    for image_id, captions in captions_dict.items():\n        for caption in captions:\n            flattened_captions.append((image_id, caption))\n\n    random.seed(random_seed)\n    random.shuffle(flattened_captions)\n\n    total_samples = len(flattened_captions)\n    train_end = int(total_samples * train_ratio)\n    val_end = train_end + int(total_samples * val_ratio)\n\n    train_samples = flattened_captions[:train_end]\n    val_samples = flattened_captions[train_end:val_end]\n    test_samples = flattened_captions[val_end:]\n\n    train_captions = {image_id: [] for image_id, _ in train_samples}\n    for image_id, caption in train_samples:\n        train_captions[image_id].append(caption)\n\n    val_captions = {image_id: [] for image_id, _ in val_samples}\n    for image_id, caption in val_samples:\n        val_captions[image_id].append(caption)\n\n    test_captions = {image_id: [] for image_id, _ in test_samples}\n    for image_id, caption in test_samples:\n        test_captions[image_id].append(caption)\n\n    return train_captions, val_captions, test_captions\n\ndef save_captions_to_json(captions, filepath):\n    with open(filepath, 'w') as file:\n        json.dump(captions, file, ensure_ascii=False, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T23:19:44.476037Z","iopub.execute_input":"2025-01-26T23:19:44.476481Z","iopub.status.idle":"2025-01-26T23:19:44.497144Z","shell.execute_reply.started":"2025-01-26T23:19:44.476438Z","shell.execute_reply":"2025-01-26T23:19:44.496005Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"captions_dict = load_captions(CAPTIONS_FILE)\ndisplay_images_with_captions(IMAGES_DIR, captions_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T23:19:44.498181Z","iopub.execute_input":"2025-01-26T23:19:44.498474Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_text(text):\n    text = text.lower()\n    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation\n    text = re.sub(r\"\\d+\", \"\", text)      # Remove numbers\n    return text.strip()\n\ndef build_tokenizer(captions_dict):\n    special_tokens = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n    word_to_index = defaultdict(lambda: special_tokens[\"<unk>\"], special_tokens.copy())\n    \n    # Tokenize captions and build vocabulary\n    vocab = set()\n    for captions in captions_dict.values():\n        for caption in captions:\n            preprocessed_caption = preprocess_text(caption)\n            vocab.update(preprocessed_caption.split())\n    \n    # Add vocabulary words to the tokenizer\n    for idx, word in enumerate(sorted(vocab), start=len(special_tokens)):\n        word_to_index[word] = idx\n    \n    return dict(word_to_index)\n\ndef caption_tokenizer_with_mask(caption, word_to_index, max_length=50):\n    tokens = [\"<sos>\"] + caption.lower().split() + [\"<eos>\"]\n    token_ids = [word_to_index.get(word, word_to_index[\"<unk>\"]) for word in tokens]\n    token_ids = token_ids[:max_length]\n    masks = [True] * len(token_ids) + [False] * (max_length - len(token_ids))\n    token_ids += [word_to_index[\"<pad>\"]] * (max_length - len(token_ids))\n    return torch.tensor(token_ids), torch.tensor(masks)\n\ndef save_tokenizer_to_json(tokenizer, filepath):\n    with open(filepath, 'w') as file:\n        json.dump(tokenizer, file, ensure_ascii=False, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T23:19:46.719454Z","iopub.execute_input":"2025-01-26T23:19:46.719831Z","iopub.status.idle":"2025-01-26T23:19:46.730599Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"word_to_index_tokenizer = build_tokenizer(captions_dict)\n\nTOKENIZER_FILE = \"tokenizer.json\"\nsave_tokenizer_to_json(word_to_index_tokenizer, TOKENIZER_FILE)\n\nprint(f\"Word to Index Tokenizer saved to {TOKENIZER_FILE}. Total tokens: {len(word_to_index_tokenizer)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T23:19:46.731734Z","iopub.execute_input":"2025-01-26T23:19:46.732039Z","iopub.status.idle":"2025-01-26T23:19:46.987387Z"}},"outputs":[{"name":"stdout","text":"Word to Index Tokenizer saved to tokenizer.json. Total tokens: 8478\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"train_captions, val_captions, test_captions = split_dataset(captions_dict)\n\nsave_captions_to_json(train_captions, \"train_captions.json\")\nsave_captions_to_json(val_captions, \"val_captions.json\")\nsave_captions_to_json(test_captions, \"test_captions.json\")\n\nprint(\"Dataset split complete:\")\nprint(f\"Training: {len(train_captions)} images\")\nprint(f\"Validation: {len(val_captions)} images\")\nprint(f\"Test: {len(test_captions)} images\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T23:19:46.990165Z","iopub.execute_input":"2025-01-26T23:19:46.990457Z","iopub.status.idle":"2025-01-26T23:19:47.161779Z"}},"outputs":[{"name":"stdout","text":"Dataset split complete:\nTraining: 8084 images\nValidation: 3166 images\nTest: 3119 images\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"class FlickrDataset(Dataset):\n    def __init__(self, captions_file, images_dir, transform=None, tokenizer=None, max_seq_length=50):\n        with open(captions_file, 'r') as file:\n            self.captions_dict = json.load(file)\n\n        self.images_dir = images_dir\n        self.transform = transform\n        self.tokenizer = tokenizer\n        self.max_seq_length = max_seq_length\n\n        self.image_caption_pairs = []\n        for image_id, captions in self.captions_dict.items():\n            image_path = os.path.join(images_dir, image_id)\n            if os.path.exists(image_path):\n                for caption in captions:\n                    self.image_caption_pairs.append((image_id, caption))\n        \n        random.shuffle(self.image_caption_pairs)\n    \n    def __len__(self):\n        return len(self.image_caption_pairs)\n\n    def __getitem__(self, idx):\n        image_id, caption = self.image_caption_pairs[idx]\n        image_path = os.path.join(self.images_dir, image_id)\n\n        image = Image.open(image_path).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n        \n        if self.tokenizer:\n            caption = self.tokenizer(caption)\n\n        return image, torch.tensor(caption) # caption\n\ndef create_dataloaders(images_dir, train_captions_file, val_captions_file, test_captions_file, batch_size, tokenizer):\n    transform_train = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(p=0.25),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n        transforms.RandomRotation(degrees=15),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n\n    val_transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n    ])\n\n    train_dataset = FlickrDataset(train_captions_file, images_dir, transform=transform_train, tokenizer=tokenizer)\n    val_dataset = FlickrDataset(val_captions_file, images_dir, transform=val_transform, tokenizer=tokenizer)\n    test_dataset = FlickrDataset(test_captions_file, images_dir, transform=val_transform, tokenizer=tokenizer)\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n\n    return train_loader, val_loader, test_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T23:19:47.163621Z","iopub.execute_input":"2025-01-26T23:19:47.163957Z","iopub.status.idle":"2025-01-26T23:19:47.176192Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# class EncoderCNN(nn.Module):\n#     def __init__(self, output_size):\n#         super(EncoderCNN, self).__init__()\n#         self.efficient_net = efficientnet_b0(pretrained=True)\n#         for param in self.efficient_net.parameters():\n#             param.requires_grad = False\n#         self.efficient_net.classifier = nn.Linear(self.efficient_net.classifier[1].in_features, output_size)\n\n#     def forward(self, images):\n#         return self.efficient_net(images)\n\n# class DecoderTransformer(nn.Module):\n#     def __init__(self, vocab_size, embed_size, num_heads, hidden_dim, num_layers, max_seq_length, pad_idx):\n#         super(DecoderTransformer, self).__init__()\n#         self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=pad_idx)\n#         self.positional_encoding = nn.Parameter(torch.zeros(1, max_seq_length, embed_size))\n#         self.transformer = nn.Transformer(\n#             d_model=embed_size, nhead=num_heads, num_encoder_layers=num_layers, num_decoder_layers=num_layers\n#         )\n#         self.fc = nn.Linear(embed_size, vocab_size)\n\n#     def forward(self, image_features, captions):\n#         captions = self.embedding(captions) + self.positional_encoding[:, :captions.size(1), :]\n#         transformer_out = self.transformer(image_features.unsqueeze(0), captions.permute(1, 0, 2))\n#         return self.fc(transformer_out)\n\n# class ImageCaptioningModel(nn.Module):\n#     def __init__(self, vocab_size, embed_size, num_heads, hidden_dim, num_layers, max_seq_length, pad_idx, encoder_output_size):\n#         super(ImageCaptioningModel, self).__init__()\n#         self.encoder = EncoderCNN(output_size=encoder_output_size)\n#         self.decoder = DecoderTransformer(\n#             vocab_size=vocab_size, \n#             embed_size=embed_size, \n#             num_heads=num_heads, \n#             hidden_dim=hidden_dim, \n#             num_layers=num_layers, \n#             max_seq_length=max_seq_length, \n#             pad_idx=pad_idx\n#         )\n\n#     def forward(self, images, captions):\n#         image_features = self.encoder(images)\n#         return self.decoder(image_features, captions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T23:19:47.177681Z","iopub.execute_input":"2025-01-26T23:19:47.178163Z","iopub.status.idle":"2025-01-26T23:19:47.199104Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_caption(model, image, tokenizer, max_length=50):\n    model.eval()\n    with torch.no_grad():\n        image_features = model.encoder(image.unsqueeze(0).to(device))\n        caption = [\"<sos>\"]\n        for _ in range(max_length):\n            input_seq = torch.tensor([tokenizer[word] for word in caption], device=device).unsqueeze(0)\n            outputs = model.decoder(image_features, input_seq)\n            predicted_word_idx = outputs.argmax(dim=-1)[:, -1].item()\n            predicted_word = {v: k for k, v in tokenizer.items()}[predicted_word_idx]\n            if predicted_word == \"<eos>\":\n                break\n            caption.append(predicted_word)\n        return \" \".join(caption[1:])\n\ndef evaluate_bleu(model, dataloader):\n    bleu_scores = []\n    for images, captions in dataloader:\n        for img, true_caption in zip(images, captions):\n            generated_caption = generate_caption(model, img)\n            score = sentence_bleu([true_caption.split()], generated_caption.split())\n            bleu_scores.append(score)\n    return sum(bleu_scores) / len(bleu_scores)\n\n# def masked_cross_entropy_loss(outputs, targets, pad_idx):\n#     loss_fn = nn.CrossEntropyLoss(ignore_index=pad_idx)\n#     outputs = outputs.view(-1, outputs.size(-1))\n#     targets = targets.view(-1)\n#     return loss_fn(outputs, targets)\n\ndef masked_cross_entropy_loss(outputs, targets, pad_idx):\n    loss_fn = nn.CrossEntropyLoss(ignore_index=pad_idx)\n    return loss_fn(outputs.permute(0, 2, 1), targets)  # Permute for [batch_size, vocab_size, seq_len]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T23:19:47.200357Z","iopub.execute_input":"2025-01-26T23:19:47.200804Z","iopub.status.idle":"2025-01-26T23:19:47.225424Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CustomTokenizer:\n    def __init__(self, captions_dict, max_length=50):\n        self.max_length = max_length\n        self.special_tokens = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n        self.word2idx = self.build_vocab(captions_dict)\n        self.idx2word = {idx: word for word, idx in self.word2idx.items()}  # Build idx2word\n\n    def build_vocab(self, captions_dict):\n        vocab = set()\n        for captions in captions_dict.values():\n            for caption in captions:\n                caption = caption.lower().strip()\n                vocab.update(caption.split())\n        \n        word2idx = defaultdict(lambda: self.special_tokens[\"<unk>\"], self.special_tokens.copy())\n        for idx, word in enumerate(sorted(vocab), start=len(self.special_tokens)):\n            word2idx[word] = idx\n        return dict(word2idx)\n    \n    def encode(self, caption):\n        tokens = [\"<sos>\"] + caption.lower().split() + [\"<eos>\"]\n        token_ids = [self.word2idx.get(word, self.special_tokens[\"<unk>\"]) for word in tokens]\n        \n        if len(token_ids) > self.max_length:\n            token_ids = token_ids[:self.max_length]\n        else:\n            token_ids += [self.special_tokens[\"<pad>\"]] * (self.max_length - len(token_ids))\n            \n        attention_mask = [False if token != self.special_tokens[\"<pad>\"] else True for token in token_ids]\n        return torch.tensor(token_ids), torch.tensor(attention_mask)\n\n    def decode(self, token_ids):\n        caption = []\n        for idx in token_ids:\n            idx = int(idx)  # Ensure it's an integer\n            if idx == self.special_tokens[\"<eos>\"]:  # Stop decoding at <eos>\n                break\n            if idx not in self.special_tokens.values():  # Skip special tokens\n                caption.append(self.idx2word[idx])\n        return \" \".join(caption).strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T23:19:47.226705Z","iopub.execute_input":"2025-01-26T23:19:47.227099Z","iopub.status.idle":"2025-01-26T23:19:47.249003Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = CustomTokenizer(captions_dict, max_length=10)\ncaption = \"A dog running in the park.\"\nencoded_caption, attention_mask = tokenizer.encode(caption)\ndecoded_caption = tokenizer.decode(encoded_caption)\n\nprint(\"Encoded Caption:\", encoded_caption)\nprint(\"Attention Mask:\", attention_mask)\nprint(\"Decoded Caption:\", decoded_caption)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T23:19:47.250571Z","iopub.execute_input":"2025-01-26T23:19:47.250957Z","iopub.status.idle":"2025-01-26T23:19:47.377096Z"}},"outputs":[{"name":"stdout","text":"Encoded Caption: tensor([   1,   85, 2240, 6265, 3792, 7655,    3,    2,    0,    0])\nAttention Mask: tensor([False, False, False, False, False, False, False, False,  True,  True])\nDecoded Caption: a dog running in the\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"class EncoderCNN(nn.Module):\n    def __init__(self, embed_size):\n        super(EncoderCNN, self).__init__()\n        efficient_net = models.efficientnet_b0(pretrained=True)\n        self.features = efficient_net.features  # Extract only feature layers\n        self.pooling = efficient_net.avgpool  # Global average pooling layer\n        for param in self.features.parameters():\n            param.requires_grad = False\n        self.fc = nn.Linear(efficient_net.classifier[1].in_features, embed_size)\n        self.relu = nn.ReLU()\n\n    def forward(self, images): # Extract image features\n        features = self.features(images)\n        features = self.pooling(features)\n        features = features.flatten(start_dim=1)  # Flatten for FC layer\n        features = self.fc(features)  # Pass through FC layer\n        features = self.relu(features)\n        return features\n\nclass DecoderTransformer(nn.Module):\n    def __init__(self, embed_size, vocab_size, num_layers, max_seq_length):\n        super(DecoderTransformer, self).__init__()\n        self.embed_size = embed_size\n        self.vocab_size = vocab_size\n        self.max_seq_length = max_seq_length\n        # Embedding layers\n        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n        self.positional_embedding = nn.Parameter(torch.zeros(1, max_seq_length, embed_size))\n        # Transformer decoder\n        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_size, nhead=8, batch_first=True)\n        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n        # Final linear layer\n        self.fc_out = nn.Linear(embed_size, vocab_size)\n\n    def forward(self, features, captions, tgt_mask):\n        # Add positional embeddings to captions\n        captions_embedding = self.word_embedding(captions) + self.positional_embedding[:, :captions.size(1), :]\n        # Decode features and captions\n        outputs = self.transformer_decoder(tgt=captions_embedding, memory=features.unsqueeze(1), tgt_mask=tgt_mask)\n        # Generate vocabulary logits\n        outputs = self.fc_out(outputs)\n        return outputs\n\nclass ImageCaptioningModel(nn.Module):\n    def __init__(self, embed_size, vocab_size, num_layers, max_seq_length):\n        super(ImageCaptioningModel, self).__init__()\n        self.encoder = EncoderCNN(embed_size)\n        self.decoder = DecoderTransformer(embed_size, vocab_size, num_layers, max_seq_length)\n\n    def forward(self, images, captions, tgt_mask):\n        features = self.encoder(images)  # Extract image features\n        outputs = self.decoder(features, captions, tgt_mask)  # Decode captions\n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T23:19:51.152064Z","iopub.execute_input":"2025-01-26T23:19:51.152397Z","iopub.status.idle":"2025-01-26T23:19:51.163394Z","shell.execute_reply.started":"2025-01-26T23:19:51.152371Z","shell.execute_reply":"2025-01-26T23:19:51.162079Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# def train_model(\n#     model, train_loader, val_loader, optimizer, pad_idx, num_epochs=50, \n#     checkpoint_path=\"checkpoint.pth\", resume=False, patience=5,\n#     weight_decay=1e-4\n# ):\n#     model.train()\n#     scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=patience-2)\n\n#     if resume and os.path.exists(checkpoint_path):\n#         start_epoch, _ = load_checkpoint(checkpoint_path, model, optimizer)\n#     else:\n#         start_epoch = 0\n            \n#     best_val_loss = float('inf')\n#     train_losses, val_losses = [], []\n#     patience_counter = 0 \n    \n#     for epoch in range(start_epoch, num_epochs):\n#         model.train()\n#         epoch_loss = 0\n#         tepoch = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n#         for batch_idx, (images, captions) in enumerate(tepoch):\n#             print(f\"captions={captions}\")\n#             images, captions = images.to(device), captions.to(device)\n            \n#             optimizer.zero_grad()\n#             outputs = model(images, captions[:, :-1])  # Exclude the last word for teacher forcing            \n#             outputs = outputs[:, :captions.size(1) - 1, :]  # Match target sequence length\n#             # outputs = model(images, captions[:, :-1])  # Exclude <eos> for input captions\n\n#             loss = masked_cross_entropy_loss(outputs, captions[:, 1:], pad_idx)  # Exclude <sos> for target captions\n#             loss.backward()\n#             optimizer.step()\n            \n#             epoch_loss += loss.item()\n#             tepoch.set_postfix(loss=f\"{loss.item():.4f}\")\n        \n#         train_losses.append(epoch_loss / len(train_loader))\n#         print(f\"Epoch {epoch+1}, Train Loss: {train_losses[-1]:.4f}\")\n\n#         model.eval()\n#         val_loss = 0\n#         with torch.no_grad():\n#             for images, captions in val_loader:\n#                 images, captions = images.to(device), captions.to(device)\n#                 # outputs = model(images, captions[:, :-1])\n#                 outputs = model(images, captions[:, :-1])\n#                 outputs = outputs[:, :captions.size(1) - 1, :]\n#                 loss = masked_cross_entropy_loss(outputs, captions[:, 1:], pad_idx)\n#                 val_loss += loss.item()\n        \n#         val_loss /= len(val_loader)\n#         val_losses.append(val_loss)\n#         print(f\"Epoch {epoch+1}, Validation Loss: {val_loss:.4f}\")\n\n#         scheduler.step(val_loss)\n\n#         if val_loss < best_val_loss:\n#             best_val_loss = val_loss\n#             patience_counter = 0 \n#             save_checkpoint(model, optimizer, epoch + 1, val_loss, checkpoint_path)\n#             # torch.save(model.state_dict(), checkpoint_path)\n#             # print(\"Model checkpoint saved!\")\n#         else:\n#             patience_counter += 1\n#             print(f\"No improvement in validation loss for {patience_counter} epoch(s).\")\n            \n#         if patience_counter >= patience:\n#             print(\"Early stopping triggered.\")\n#             break\n\n#     return train_losses, val_losses\ndef train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n    model = model.to(device)\n    train_losses, val_losses = [], []\n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0.0\n        for images, captions in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n            images, captions = images.to(device), captions.to(device)\n            optimizer.zero_grad()\n            # Generate masks for the captions\n            tgt_mask = nn.Transformer.generate_square_subsequent_mask(captions.size(1)).to(device)\n            # Forward pass\n            outputs = model(images, captions, tgt_mask)\n            loss = criterion(outputs.view(-1, outputs.size(-1)), captions.view(-1))\n            # Backward pass and optimization\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for images, captions in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}/{num_epochs}\"):\n                images, captions = images.to(device), captions.to(device)\n                tgt_mask = nn.Transformer.generate_square_subsequent_mask(captions.size(1)).to(device)\n                outputs = model(images, captions, tgt_mask)\n                loss = criterion(outputs.view(-1, outputs.size(-1)), captions.view(-1))\n                val_loss += loss.item()\n                \n        train_losses.append(train_loss / len(train_loader))\n        val_losses.append(val_loss / len(val_loader))\n        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]}, Val Loss: {val_losses[-1]}\")\n    return train_losses, val_losses","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T23:19:51.181798Z","iopub.execute_input":"2025-01-26T23:19:51.182140Z","iopub.status.idle":"2025-01-26T23:19:51.193490Z","shell.execute_reply.started":"2025-01-26T23:19:51.182115Z","shell.execute_reply":"2025-01-26T23:19:51.192022Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"with open(TOKENIZER_FILE, \"r\") as file:\n    word_to_index = json.load(file)\n\ntrain_loader, val_loader, test_loader = create_dataloaders(\n    IMAGES_DIR,\n    \"train_captions.json\",\n    \"val_captions.json\",\n    \"test_captions.json\",\n    BATCH_SIZE,\n    lambda caption: caption_tokenizer_with_mask(caption, word_to_index)\n)\n\nprint(f\"Training batches: {len(train_loader)}\")\nprint(f\"Validation batches: {len(val_loader)}\")\nprint(f\"Testing batches: {len(test_loader)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T23:19:51.197082Z","iopub.execute_input":"2025-01-26T23:19:51.197529Z","iopub.status.idle":"2025-01-26T23:20:09.767151Z","shell.execute_reply.started":"2025-01-26T23:19:51.197488Z","shell.execute_reply":"2025-01-26T23:20:09.766078Z"}},"outputs":[{"name":"stdout","text":"Training batches: 476\nValidation batches: 60\nTesting batches: 60\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# Hyperparameters\nEMBED_SIZE = 256\nNUM_LAYERS = 6\nMAX_SEQ_LENGTH = 50\nVOCAB_SIZE = len(word_to_index)  # Vocabulary size\n# VOCAB_SIZE = len(word_to_index_tokenizer)  # Vocabulary size\nNUM_EPOCHS = 10\nLEARNING_RATE = 1e-4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T23:20:09.768281Z","iopub.execute_input":"2025-01-26T23:20:09.768566Z","iopub.status.idle":"2025-01-26T23:20:09.773703Z","shell.execute_reply.started":"2025-01-26T23:20:09.768521Z","shell.execute_reply":"2025-01-26T23:20:09.772343Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Initialize the model\nmodel = ImageCaptioningModel(\n    embed_size=EMBED_SIZE,\n    vocab_size=VOCAB_SIZE,\n    num_layers=NUM_LAYERS,\n    max_seq_length=MAX_SEQ_LENGTH\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T23:20:09.775491Z","iopub.execute_input":"2025-01-26T23:20:09.775874Z","iopub.status.idle":"2025-01-26T23:20:10.471064Z","shell.execute_reply.started":"2025-01-26T23:20:09.775847Z","shell.execute_reply":"2025-01-26T23:20:10.470030Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n100%|██████████| 20.5M/20.5M [00:00<00:00, 156MB/s]\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# vocab_size = len(word_to_index)\n# embed_size = 512\n# num_heads = 8\n# hidden_dim = 512\n# num_layers = 4\n# max_seq_length = 50\n# pad_idx = word_to_index[\"<pad>\"]\n# encoder_output_size = 512\n\n# model = ImageCaptioningModel(\n#     vocab_size, embed_size, num_heads, hidden_dim, num_layers, max_seq_length, pad_idx, encoder_output_size\n# ).to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T23:20:10.472725Z","iopub.execute_input":"2025-01-26T23:20:10.473006Z","iopub.status.idle":"2025-01-26T23:20:10.477264Z","shell.execute_reply.started":"2025-01-26T23:20:10.472984Z","shell.execute_reply":"2025-01-26T23:20:10.476035Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"PAD_IDX = word_to_index_tokenizer[\"<pad>\"]\n\n# Loss function (ignores padding tokens)\ncriterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n\n# Optimizer and learning rate scheduler\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\nscheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T23:20:39.749524Z","iopub.execute_input":"2025-01-26T23:20:39.749954Z","iopub.status.idle":"2025-01-26T23:20:39.760944Z","shell.execute_reply.started":"2025-01-26T23:20:39.749922Z","shell.execute_reply":"2025-01-26T23:20:39.759422Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"train_losses, val_losses = train_model(\n    model=model,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    criterion=criterion,\n    optimizer=optimizer,\n    num_epochs=NUM_EPOCHS,\n    device=device\n)\nplot_losses(train_losses, val_losses)\ntorch.save(model.state_dict(), \"image_captioning_model.pth\")\nprint(\"Model saved as 'image_captioning_model.pth'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T23:20:41.461020Z","iopub.execute_input":"2025-01-26T23:20:41.461353Z","iopub.status.idle":"2025-01-26T23:20:41.760835Z","shell.execute_reply.started":"2025-01-26T23:20:41.461328Z","shell.execute_reply":"2025-01-26T23:20:41.758720Z"}},"outputs":[{"name":"stderr","text":"Training Epoch 1/10:   0%|          | 0/476 [00:00<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-91af3495e2e2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_losses, val_losses = train_model(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mval_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-a0051683de5d>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Training Epoch {epoch+1}/{num_epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1342\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1344\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1368\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 309, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-9-0e5d9d0c9d67>\", line 34, in __getitem__\n    return image, torch.tensor(caption) # caption\nTypeError: only integer tensors of a single element can be converted to an index\n"],"ename":"TypeError","evalue":"Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 309, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-9-0e5d9d0c9d67>\", line 34, in __getitem__\n    return image, torch.tensor(caption) # caption\nTypeError: only integer tensors of a single element can be converted to an index\n","output_type":"error"}],"execution_count":24},{"cell_type":"code","source":"def generate_caption(model, tokenizer, image, device, max_seq_length=20):\n    model.eval()\n    with torch.no_grad():\n        image = image.to(device).unsqueeze(0)  # Ensure image is a tensor and on the correct device\n        features = model.encoder(image)\n        caption = [tokenizer.special_tokens[\"<sos>\"]]\n        for _ in range(max_seq_length):\n            tgt = torch.tensor(caption).unsqueeze(0).to(device)  # Target tokens as input\n            tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size(1)).to(device)\n            outputs = model.decoder(features, tgt, tgt_mask)\n            next_token = outputs.argmax(dim=-1)[:, -1].item()  # Get the next token\n            caption.append(next_token)\n            if next_token == tokenizer.special_tokens[\"<eos>\"]:  # Stop if <eos> is reached\n                break\n        return tokenizer.decode(caption)\n\ndef visualize_results(model, dataloader, tokenizer, num_images=5):\n    model = model.to(device)\n    for images, _ in dataloader:\n        images = images[:num_images].to(device)\n        captions = [generate_caption(model, tokenizer, image) for image in images]\n\n        plt.figure(figsize=(5, num_images * 5))\n        for i in range(num_images):\n            ax = plt.subplot(num_images, 1, i + 1)\n            ax.imshow(images[i].permute(1, 2, 0).cpu().numpy())  # Unnormalize the image\n            ax.axis(\"off\")\n            ax.set_title(captions[i], fontsize=12, wrap=True)\n\n        plt.tight_layout()\n        plt.show()\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T23:20:10.637725Z","iopub.status.idle":"2025-01-26T23:20:10.638055Z","shell.execute_reply":"2025-01-26T23:20:10.637927Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"visualize_results(model, test_loader, tokenizer, num_images=5) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T23:20:10.639190Z","iopub.status.idle":"2025-01-26T23:20:10.639643Z","shell.execute_reply":"2025-01-26T23:20:10.639429Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bleu_score = evaluate_bleu(model, test_loader, word_to_index)\nprint(f\"BLEU Score: {bleu_score:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T23:20:10.640793Z","iopub.status.idle":"2025-01-26T23:20:10.641127Z","shell.execute_reply":"2025-01-26T23:20:10.640997Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}