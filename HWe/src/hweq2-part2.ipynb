{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install rouge-score\n\nimport os\nimport re\nimport cv2\nimport nltk\nimport json\nimport random\nimport numpy as np\nfrom tqdm import tqdm\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom collections import defaultdict\nfrom nltk.corpus import stopwords\nfrom rouge_score import rouge_scorer\nfrom nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.models as models\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.models.efficientnet import EfficientNet_B0_Weights\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"IMAGES_DIR = \"/kaggle/input/flickr8k/Images\"\nCAPTIONS_FILE = \"/kaggle/input/flickr8k/captions.txt\"\n\nBATCH_SIZE = 32\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_captions(captions_file):\n    captions_dict = {}\n    with open(captions_file, 'r') as file:\n        for line in file:\n            tokens = line.strip().split(\",\") \n            if len(tokens) == 2:\n                image_id, caption = tokens\n                if image_id not in captions_dict:\n                    captions_dict[image_id] = []\n                captions_dict[image_id].append(caption)\n    return captions_dict\n\ndef display_images_with_captions(images_dir, captions_dict, num_images=3, image_size=(224, 224)):\n    images = list(captions_dict.keys())\n    if len(images) < num_images: raise ValueError(f\"Doesn't Exist.\")    \n    selected_images = random.sample(images, num_images)\n    plt.figure(figsize=(10 * num_images, 10))\n    for i, image in enumerate(selected_images):\n        img_path = os.path.join(images_dir, image)\n        img = Image.open(img_path)        \n        plt.subplot(num_images, 1, i + 1)\n        plt.imshow(img)\n        plt.axis('off')\n        captions = \"\\n\".join(captions_dict[image]) \n        plt.title(captions, fontsize=10, loc='center', wrap=True)\n    plt.tight_layout()\n    plt.show()\n\ndef plot_training_losses(train_losses, val_losses):\n    plt.figure(figsize=(10, 6))\n    plt.plot(range(1, len(train_losses) + 1), train_losses, marker='o', label='Training Loss')\n    plt.plot(range(1, len(val_losses) + 1), val_losses, marker='x', label='Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.title('Training and Validation Loss Over Epochs')\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n\n# def display_random_images_with_captions(images_dir, captions_dict, num_images=5):\n#     selected_images = random.sample(list(captions_dict.keys()), num_images)\n#     plt.figure(figsize=(15, 10))\n\n#     for i, image_id in enumerate(selected_images):\n#         img_path = f\"{images_dir}/{image_id}\"\n#         img = Image.open(img_path)\n\n#         plt.subplot(num_images, 1, i + 1)\n#         # plt.subplot(1, num_images, i + 1)\n#         plt.imshow(img)\n#         plt.axis(\"off\")\n#         plt.title(captions_dict[image_id], fontsize=10)\n    \n#     plt.tight_layout()\n#     plt.show()\n        \ndef plot_caption_length_variation(captions_dict):\n    lengths = []\n    for captions in captions_dict.values():\n        lengths.extend([len(caption.split()) for caption in captions])\n    plt.figure(figsize=(15, 10))\n    plt.scatter(range(len(lengths)), lengths, alpha=0.5)\n    plt.title(\"Variation in Caption Lengths\", fontsize=14)\n    plt.xlabel(\"Caption Index\", fontsize=12)\n    plt.ylabel(\"Caption Length (Number of Words)\", fontsize=12)\n    plt.grid(True, alpha=0.3)\n    plt.show()\n\ndef plot_word_frequency_histogram(captions_dict, top_n=20):\n    word_counter = Counter()\n    for captions in captions_dict.values():\n        for caption in captions:\n            preprocessed_caption = preprocess_text(caption)\n            word_counter.update(preprocessed_caption.split())\n    most_common_words = word_counter.most_common(top_n)\n    words, counts = zip(*most_common_words)\n    plt.figure(figsize=(12, 6))\n    plt.bar(words, counts, color='skyblue')\n    plt.title(f\"Top {top_n} Most Frequent Words\", fontsize=14)\n    plt.xlabel(\"Words\", fontsize=12)\n    plt.ylabel(\"Frequency\", fontsize=12)\n    plt.xticks(rotation=45, fontsize=10)\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def split_dataset(captions_dict, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, random_seed=42):\n    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \"Ratios must sum to 1.\"\n\n    flattened_captions = []\n    for image_id, captions in captions_dict.items():\n        for caption in captions:\n            flattened_captions.append((image_id, caption))\n\n    random.seed(random_seed)\n    random.shuffle(flattened_captions)\n\n    total_samples = len(flattened_captions)\n    train_end = int(total_samples * train_ratio)\n    val_end = train_end + int(total_samples * val_ratio)\n\n    train_samples = flattened_captions[:train_end]\n    val_samples = flattened_captions[train_end:val_end]\n    test_samples = flattened_captions[val_end:]\n\n    train_captions = {image_id: [] for image_id, _ in train_samples}\n    for image_id, caption in train_samples:\n        train_captions[image_id].append(caption)\n\n    val_captions = {image_id: [] for image_id, _ in val_samples}\n    for image_id, caption in val_samples:\n        val_captions[image_id].append(caption)\n\n    test_captions = {image_id: [] for image_id, _ in test_samples}\n    for image_id, caption in test_samples:\n        test_captions[image_id].append(caption)\n\n    return train_captions, val_captions, test_captions\n\ndef save_captions_to_json(captions, filepath):\n    with open(filepath, 'w') as file:\n        json.dump(captions, file, ensure_ascii=False, indent=4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"captions_dict = load_captions(CAPTIONS_FILE)\ndisplay_images_with_captions(IMAGES_DIR, captions_dict)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"nltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\n\ndef preprocess_text(text):\n    text = text.lower()\n    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation\n    text = re.sub(r\"\\d+\", \"\", text)      # Remove numbers\n    words = text.split()                 # Tokenize text\n    words = [word for word in words if word not in stop_words]  # Remove stop words\n    return \" \".join(words).strip()\n\ntoken_to_index = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n\nindex_to_word = {v: k for k, v in token_to_index.items()}  # Reverse the token_to_index mapping\n\nvocab = set()\nall_words_dict = {}\n\nfor captions in captions_dict.values():\n    for caption in captions:\n        caption = caption.lower()\n        caption = re.sub(r\"[^\\w\\s]\", \"\", caption)  # Remove punctuation\n        caption = re.sub(r\"\\d+\", \"\", caption)      # Remove numbers\n        # preprocessed_caption = preprocess_text(caption)\n        for word in caption.split():\n            if word not in token_to_index:  \n                idx = len(token_to_index)\n                token_to_index[word] = idx\n                index_to_word[idx] = word\n                vocab.add(word)\n            \n            if word not in all_words_dict:\n                all_words_dict[word] = len(all_words_dict)\n\nword_to_index_dictionary = dict(token_to_index)\nindex_to_word_dictionary = dict(index_to_word)\n\nprint(\"Word to Index:\", len(word_to_index_dictionary))\nprint(\"Index to Word:\", len(index_to_word_dictionary))\nprint(\"All Words Dictionary:\", len(all_words_dict))  # Output the size of the all_words_dict","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TOKENIZER_FILE = \"tokenizer.json\"\nwith open(TOKENIZER_FILE, 'w') as file:\n    json.dump(word_to_index_dictionary, file, ensure_ascii=False, indent=4)\nprint(f\"Word to Index Tokenizer saved to {TOKENIZER_FILE}. Total tokens: {len(word_to_index_dictionary)}\")\n\n# with open(TOKENIZER_FILE, \"r\") as file:\n#     word_to_index_dictionary = json.load(file)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_captions, val_captions, test_captions = split_dataset(captions_dict)\n\nsave_captions_to_json(train_captions, \"train_captions.json\")\nsave_captions_to_json(val_captions, \"val_captions.json\")\nsave_captions_to_json(test_captions, \"test_captions.json\")\n\nprint(\"Dataset split complete:\")\nprint(f\"Training: {len(train_captions)} images\")\nprint(f\"Validation: {len(val_captions)} images\")\nprint(f\"Test: {len(test_captions)} images\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FlickrDataset(Dataset):\n    def __init__(self, captions_file, images_dir, transform=None, tokenizer=None):\n        with open(captions_file, 'r') as file:\n            self.captions_dict = json.load(file)\n        self.images_dir = images_dir\n        self.transform = transform\n        self.tokenizer = tokenizer\n        self.image_ids = [img for img in self.captions_dict.keys() if os.path.exists(os.path.join(images_dir, img))]\n     \n    def __len__(self):\n        return len(self.image_ids)\n\n    def __getitem__(self, idx):\n        image_id = self.image_ids[idx]\n        image_path = os.path.join(self.images_dir, image_id)\n\n        # Load image\n        image = Image.open(image_path).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n\n        # Randomly select a caption\n        caption = random.choice(self.captions_dict[image_id])\n        caption = preprocess_text(caption)\n\n        if self.tokenizer:\n            caption = self.tokenizer(caption)\n\n        return image, caption","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_caption_length_variation(captions_dict)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_word_frequency_histogram(captions_dict, top_n=20)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def pad_caption(caption, word_to_index, max_length=15):\n    tokens = [\"<sos>\"] + caption.lower().split() + [\"<eos>\"]\n    # token_ids = [word_to_index.get(word, word_to_index[\"<unk>\"]) for word in tokens]\n    token_ids = []\n    for word in tokens:\n        token_id = word_to_index.get(word, word_to_index[\"<unk>\"])\n        token_ids.append(token_id)\n\n    if len(token_ids) < max_length:\n        token_ids += [word_to_index[\"<pad>\"]] * (max_length - len(token_ids))\n    else:\n        token_ids = token_ids[:max_length]\n    return torch.tensor(token_ids)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\nval_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ndenormalize = transforms.Compose([\n    transforms.Normalize(mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225], \n                         std=[1/0.229, 1/0.224, 1/0.225])\n])\n\ntrain_dataset = FlickrDataset(\"train_captions.json\", IMAGES_DIR, transform=transform, tokenizer=lambda caption: pad_caption(caption, word_to_index_dictionary))\nval_dataset = FlickrDataset(\"val_captions.json\", IMAGES_DIR, transform=val_transform, tokenizer=lambda caption: pad_caption(caption, word_to_index_dictionary))\ntest_dataset = FlickrDataset(\"test_captions.json\", IMAGES_DIR, transform=val_transform, tokenizer=lambda caption: pad_caption(caption, word_to_index_dictionary))\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n\nprint(f\"Training batches: {len(train_loader)}\")\nprint(f\"Validation batches: {len(val_loader)}\")\nprint(f\"Testing batches: {len(test_loader)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def masked_cross_entropy_loss(predictions, targets, pad_token=0):\n    batch_size, seq_len, vocab_size = predictions.shape\n    predictions = predictions.reshape(-1, vocab_size)  # Flatten logits\n    targets = targets.reshape(-1)  # Flatten targets\n    mask = targets != pad_token\n    loss_fn = nn.CrossEntropyLoss(ignore_index=pad_token, label_smoothing=0.1) # F.cross_entropy\n    loss = loss_fn(predictions, targets) # , reduction='none'\n    loss = loss * mask\n    return loss.sum() / mask.sum()\n\ndef save_checkpoint(model, optimizer, epoch, loss, checkpoint_path=\"checkpoint.pth\"):\n    checkpoint = {\n        \"model_state_dict\": model.state_dict(),\n        \"optimizer_state_dict\": optimizer.state_dict(),\n        \"epoch\": epoch,\n        \"loss\": loss,\n    }\n    torch.save(checkpoint, checkpoint_path)\n\ndef load_checkpoint(checkpoint_path, model, optimizer):\n    checkpoint = torch.load(checkpoint_path)\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n    epoch = checkpoint[\"epoch\"]\n    loss = checkpoint[\"loss\"]\n    print(f\"Checkpoint loaded from epoch {epoch}\")\n    return epoch, loss\n\ndef generate_caption(model, image, tokenizer, max_length=15):\n    model.eval() \n\n    index_to_word = {index: word for word, index in tokenizer.items()}\n    start_token = tokenizer[\"<sos>\"]\n    end_token = tokenizer[\"<eos>\"]\n\n    with torch.no_grad():\n        features = model.encoder(image.unsqueeze(0).to(device))\n\n        caption = [start_token]  # Begin with <sos>\n        for _ in range(max_length):\n            caption_tensor = torch.tensor([caption], device=device)  # Shape: (1, current_seq_len)\n            outputs = model.decoder(features, caption_tensor)\n            next_word_idx = outputs[0, -1].argmax().item()\n            caption.append(next_word_idx)\n            if next_word_idx == end_token:\n                break\n\n    words = [index_to_word[idx] for idx in caption if idx != start_token and idx != end_token]\n    return \" \".join(words)\n\n# def visualize_results(model, dataloader, tokenizer, num_images=5):\n#     model.eval()\n#     for images, _ in dataloader:\n#         images = images[:num_images].to(device)\n#         denorm_images = torch.stack([denormalize(img) for img in images])\n#         captions = [generate_caption(model, image, tokenizer) for image in images]\n#         plt.figure(figsize=(5, num_images * 5))\n#         for i in range(num_images):\n#             ax = plt.subplot(num_images, 1, i + 1)\n#             img = denorm_images[i].permute(1, 2, 0).cpu().numpy()\n#             img = img.clip(0, 1)  # Ensure pixel values are in [0,1] range\n#             ax.imshow(img)\n#             ax.axis(\"off\")  # Remove axes\n#             ax.set_title(captions[i], fontsize=12, wrap=True)\n#         plt.tight_layout()\n#         plt.show()\n#         break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_attention(image, attention_weights, caption, vocab):\n    \"\"\"\n    Visualizes the attention maps for each generated word in the caption.\n    \"\"\"\n    fig, axes = plt.subplots(1, len(caption), figsize=(20, 5))\n    for idx, ax in enumerate(axes):\n        attn_map = attention_weights[idx].detach().cpu().numpy().reshape(14, 14)  # Assuming a 14x14 feature map\n        ax.imshow(image.permute(1, 2, 0).cpu().numpy())\n        ax.imshow(attn_map, cmap='jet', alpha=0.6)\n        ax.axis('off')\n        ax.set_title(vocab[caption[idx].item()] if caption[idx].item() in vocab else \"<unk>\")\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(\n    model, train_loader, val_loader, optimizer, num_epochs=50, \n    checkpoint_path=\"checkpoint.pth\", resume=False, patience=5,\n    weight_decay=1e-4\n):\n    model.train()\n    if resume and os.path.exists(checkpoint_path):\n        start_epoch, _ = load_checkpoint(checkpoint_path, model, optimizer)\n    else:\n        start_epoch = 0\n        \n    train_losses = []\n    val_losses = []\n    best_val_loss = float('inf')\n    patience_counter = 0 \n\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n    for epoch in range(start_epoch, num_epochs):\n        model.train()\n        epoch_loss = 0\n        tepoch = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n        \n        for batch_idx, (images, captions) in enumerate(tepoch):\n            images, captions = images.to(device), captions.to(device)\n            optimizer.zero_grad()\n\n            outputs, attention_weights = model(images, captions[:, :-1])  # Exclude the last word for teacher forcing            \n            outputs = outputs[:, :captions.size(1) - 1, :]  # Match target sequence length\n\n            loss = masked_cross_entropy_loss(outputs, captions[:, 1:])  # Exclude <sos> for target\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n            tepoch.set_postfix(loss=f\"{loss.item():.4f}\")\n\n        train_losses.append(epoch_loss / len(train_loader))\n        scheduler.step(epoch_loss / len(train_loader))\n        print(f\"Epoch {epoch+1}, Train Loss: {train_losses[-1]:.4f}\")\n\n        # Validation step\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for images, captions in val_loader:\n                images, captions = images.to(device), captions.to(device)\n                outputs, attention_weights = model(images, captions[:, :-1])\n                outputs = outputs[:, :captions.size(1) - 1, :]\n                loss = masked_cross_entropy_loss(outputs, captions[:, 1:])\n                val_loss += loss.item()\n\n        val_loss /= len(val_loader)\n        val_losses.append(val_loss)\n        print(f\"Epoch {epoch+1}, Validation Loss: {val_loss:.4f}\")\n\n        # Save checkpoint\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0 \n            save_checkpoint(model, optimizer, epoch + 1, val_loss, checkpoint_path)\n        else:\n            patience_counter += 1\n            print(f\"No improvement in validation loss for {patience_counter} epoch(s).\")\n\n        # Show sample image with caption at the end of each epoch\n        show_sample_caption(model, val_loader)\n\n        # Early stopping\n        if patience_counter >= patience:\n            print(\"Early stopping triggered.\")\n            break\n\n    return train_losses, val_losses\n\n\ndef show_sample_caption(model, val_loader):\n    \"\"\" Display a sample image with predicted and real captions \"\"\"\n    model.eval()\n    with torch.no_grad():\n        # Get a random sample from validation set\n        images, captions = next(iter(val_loader))\n        image = images[0].unsqueeze(0).to(device)  # Select one image\n        real_caption = captions[0]  # Select corresponding caption\n\n        # Generate caption\n        predicted_caption = generate_caption(model, image)\n\n        # Convert image to display format\n        img = image.squeeze(0).cpu().permute(1, 2, 0).numpy()\n        img = (img - img.min()) / (img.max() - img.min())  # Normalize for display\n\n        # Plot image with captions\n        plt.figure(figsize=(6, 6))\n        plt.imshow(img)\n        plt.axis(\"off\")\n        plt.title(f\"Real: {' '.join(real_caption)}\\nPred: {predicted_caption}\", fontsize=10)\n        plt.show()\n\n\ndef generate_caption(model, image, max_length=20):\n    \"\"\" Generate a caption using greedy search (replace with beam search if needed) \"\"\"\n    model.eval()\n    with torch.no_grad():\n        caption = [\"<sos>\"]\n        for _ in range(max_length):\n            input_seq = torch.tensor([vocab.word2idx[word] for word in caption]).unsqueeze(0).to(device)\n            output, _ = model(image, input_seq)\n            next_word_idx = output.argmax(-1)[:, -1].item()\n            next_word = vocab.idx2word[next_word_idx]\n\n            if next_word == \"<eos>\":\n                break\n            caption.append(next_word)\n\n    return \" \".join(caption[1:])  # Remove <sos>\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class EncoderCNN(nn.Module):\n    def __init__(self, embed_size):\n        super(EncoderCNN, self).__init__()\n        self.model = models.efficientnet_b0(pretrained=True)\n        self.model.classifier = nn.Identity()\n        self.fc = nn.Linear(1280, embed_size)\n        self.relu = nn.ReLU()\n    \n    def forward(self, images):\n        features = self.model(images)  # (batch_size, 1280)\n        features = self.fc(features)   # (batch_size, embed_size)\n        features = self.relu(features)\n        # Ensure output shape is (batch_size, num_regions, feature_dim)\n        features = features.unsqueeze(1)  # Change (batch_size, embed_size) to (batch_size, 1, embed_size)\n        return features\n\n\nclass Attention(nn.Module):\n    def __init__(self, feature_dim, hidden_dim):\n        super(Attention, self).__init__()\n        self.attn = nn.Linear(feature_dim + hidden_dim, hidden_dim)\n        self.v = nn.Linear(hidden_dim, 1)\n\n    def forward(self, features, hidden):\n        hidden = hidden.unsqueeze(1).expand(-1, features.size(1), -1)\n        attn_input = torch.cat((features, hidden), dim=2)\n        attn_weights = F.softmax(self.v(torch.tanh(self.attn(attn_input))), dim=1)\n        context = (attn_weights * features).sum(dim=1)\n        return context, attn_weights\n\n\nclass DecoderRNN(nn.Module):\n    def __init__(self, embed_size, hidden_size, vocab_size, feature_dim, num_layers=1):\n        super(DecoderRNN, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.attention = Attention(feature_dim, hidden_size)\n        self.lstm = nn.LSTM(embed_size + feature_dim, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, vocab_size)\n        self.softmax = nn.LogSoftmax(dim=2)\n\n    def forward(self, features, captions):\n        embeddings = self.embedding(captions)  # (batch, max_seq_len, embed_dim)\n        h, c = torch.zeros(1, features.size(0), self.lstm.hidden_size).to(features.device), \\\n               torch.zeros(1, features.size(0), self.lstm.hidden_size).to(features.device)\n\n        outputs = []\n        attention_weights = []  # To store attention weights for visualization\n\n        for t in range(captions.size(1)):  # Iterate over each time step\n            context, attn_weights = self.attention(features, h.squeeze(0))  # Compute attention-weighted feature\n            attention_weights.append(attn_weights)  # Save attention weights for visualization\n            lstm_input = torch.cat((embeddings[:, t, :], context), dim=1).unsqueeze(1)  # Merge context and word embedding\n            output, (h, c) = self.lstm(lstm_input, (h, c))  # LSTM step\n            output = self.fc(output)  # Final prediction\n            outputs.append(output)\n\n        outputs = torch.cat(outputs, dim=1)  # Convert list to tensor\n        attention_weights = torch.stack(attention_weights, dim=1)  # (batch_size, seq_len, num_regions)\n        return self.softmax(outputs), attention_weights\n\nclass ImageCaptioningModel(nn.Module):\n    def __init__(self, vocab_size, embed_size=512, hidden_size=512, num_layers=1, dropout=0.5):\n        super(ImageCaptioningModel, self).__init__()\n        self.encoder = EncoderCNN(embed_size).to(device)\n        self.decoder = DecoderRNN(embed_size, hidden_size, vocab_size, feature_dim=embed_size, num_layers=num_layers).to(device)\n\n    def forward(self, images, captions):\n        features = self.encoder(images)  # Extract image features\n        outputs = self.decoder(features, captions)  # Generate captions\n        return outputs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vocab_size = len(word_to_index_dictionary)\nlearning_rate = 0.001\nweight_decay = 1e-4\n\nmodel = ImageCaptioningModel(\n    vocab_size, embed_size=256, hidden_size=512, num_layers=1, dropout=0.5\n).to(device)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_losses, val_losses = train_model(\n    model, train_loader, val_loader, optimizer, num_epochs=50, \n    checkpoint_path=\"caption_model.pth\", patience=15\n)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-02T16:58:01.376Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_training_losses(train_losses, val_losses)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-02T16:58:01.376Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval()\nimages_samples, references, hypotheses = [], [], []\nindex_to_word = {index: word for word, index in word_to_index_dictionary.items()}\nspecial_tokens = {word_to_index_dictionary[\"<sos>\"], word_to_index_dictionary[\"<eos>\"], word_to_index_dictionary[\"<pad>\"]}\n\nwith torch.no_grad():\n    for images, captions in test_loader:\n        images = images.to(device)\n        captions = captions.to(device)\n        predicted_captions = [generate_caption(model, image, word_to_index_dictionary) for image in images]\n        true_captions = []\n        for caption in captions.cpu().numpy():\n            words = [index_to_word[idx] for idx in caption if idx not in special_tokens]\n            true_captions.append(\" \".join(words))\n        references.extend([[ref] for ref in true_captions])\n        hypotheses.extend(predicted_captions)\n        images_samples.extend(images.cpu())\n\nsmoothie = SmoothingFunction().method1\nbleu1 = corpus_bleu(references, hypotheses, weights=(1.0, 0, 0, 0), smoothing_function=smoothie)\nbleu2 = corpus_bleu(references, hypotheses, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothie)\nbleu3 = corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothie)\nbleu4 = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie)\n\nscorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\nrouge_l_scores = [scorer.score(\" \".join(refs[0]), \" \".join(hyp))['rougeL'].fmeasure for refs, hyp in zip(references, hypotheses)]\nrouge_l = np.mean(rouge_l_scores)\n\nprint(f\"Evaluation on test set:\")\nprint(f\"BLEU-1 Score: {bleu1:.4f}\")\nprint(f\"BLEU-2 Score: {bleu2:.4f}\")\nprint(f\"BLEU-3 Score: {bleu3:.4f}\")\nprint(f\"BLEU-4 Score: {bleu4:.4f}\")\nprint(f\"ROUGE-L Score: {rouge_l:.4f}\")\n\nselected_indices = random.sample(range(len(images_samples)), min(5, len(images_samples)))    \nfig, axes = plt.subplots(len(selected_indices), 1, figsize=(8, len(selected_indices) * 3))\nif len(selected_indices) == 1: axes = [axes]\nfor i, idx in enumerate(selected_indices):\n    img = images_samples[idx].permute(1, 2, 0).numpy()\n    img = (img - img.min()) / (img.max() - img.min())  # Normalize image for display\n    ref_caption = references[idx][0]\n    hyp_caption = hypotheses[idx]\n    axes[i].imshow(img)\n    axes[i].axis(\"off\")\n    axes[i].set_title(f\"Real: {ref_caption}\\nPred: {hyp_caption}\", fontsize=10)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-02T16:58:01.376Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_attention(image, attention_weights, captions, vocab):\n    \"\"\"\n    Visualizes the attention maps for each generated word in the caption.\n    \"\"\"\n    fig, axes = plt.subplots(1, len(captions), figsize=(20, 5))\n    for idx, ax in enumerate(axes):\n        attn_map = attention_weights[0, idx].reshape(14, 14).detach().cpu().numpy()  # Assume 14x14 feature map\n        ax.imshow(image.permute(1, 2, 0).cpu().numpy())\n        ax.imshow(attn_map, cmap='jet', alpha=0.6)  # Overlay attention map\n        ax.axis('off')\n        ax.set_title(vocab[captions[idx].item()])\n    plt.show()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-02T16:58:01.376Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_attention(image, alphas, caption, index_to_word, save_path=None):\n    image = image.permute(1, 2, 0).cpu().numpy()  # Convert image to HWC format\n    image = np.clip(image * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406]), 0, 1)  # Unnormalize\n\n    for t, alpha in enumerate(alphas):\n        plt.figure(figsize=(8, 8))  # Increase figure size for better clarity\n        plt.imshow(image)\n\n        # Reshape alpha and add batch/channel dimensions for interpolation\n        alpha = alpha.view(1, 1, 7, 7)  # Reshape to (N, C, H, W)\n        alpha = F.interpolate(alpha, size=(image.shape[0], image.shape[1]), mode=\"bilinear\", align_corners=False)\n        alpha = alpha.squeeze().cpu().detach().numpy()  # Remove dimensions and convert to numpy\n\n        plt.imshow(alpha, cmap=\"jet\", alpha=0.5)  # Overlay heat map\n        word = index_to_word.get(caption[t], \"<unk>\")\n        plt.title(f\"Word: {word}\", fontsize=16)  # Larger font for better readability\n        plt.axis(\"off\")\n        if save_path:\n            plt.savefig(f\"{save_path}_t{t}.png\", bbox_inches=\"tight\", dpi=200)  # Save with high resolution\n        plt.show()\n\ndef generate_and_visualize_attention(model, test_loader, tokenizer, index_to_word, max_length=15, save_path=None):\n    model.eval()\n    sample_image, _ = next(iter(test_loader))  # Get a single batch from the test loader\n    sample_image = sample_image[0].unsqueeze(0).to(device)  # Take the first image from the batch\n\n    start_token = tokenizer[\"<sos>\"]\n    end_token = tokenizer[\"<eos>\"]\n\n    with torch.no_grad():\n        features = model.encoder(sample_image)\n        caption = [start_token]\n        alphas = []\n\n        for _ in range(max_length):\n            caption_tensor = torch.tensor([caption], device=device)\n            outputs, alpha = model.decoder(features, caption_tensor)\n            next_word_idx = outputs[0, -1].argmax().item()\n            alphas.append(alpha[0, -1])\n            caption.append(next_word_idx)\n            if next_word_idx == end_token:\n                break\n\n    words = [index_to_word[idx] for idx in caption if idx not in {start_token, end_token}]\n    caption_text = \" \".join(words)\n    print(f\"Generated Caption: {caption_text}\")\n\n    visualize_attention(sample_image[0], alphas, caption, index_to_word, save_path)\n\n\n\n\n\n\n    # outputs, attention_weights = model(images, captions[:, :-1])  # Exclude the last word for teacher forcing            \n    # outputs = outputs[:, :captions.size(1) - 1, :]  # Match target sequence length\n\n    # loss = masked_cross_entropy_loss(outputs, captions[:, 1:])  # Exclude <sos> for target\n    # loss.backward()\n    # optimizer.step()\n    # epoch_loss += loss.item()\n    # tepoch.set_postfix(loss=f\"{loss.item():.4f}\")\n\n    # # Visualize attention for the first batch of every epoch\n    # if batch_idx == 0:\n    #     visualize_attention(images[0], attention_weights[0], captions[0], word_to_index_dictionary)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# generate_and_visualize_attention(\n#     model, test_loader, tokenizer=word_to_index_tokenizer,\n#     index_to_word={index: word for word, index in word_to_index_tokenizer.items()},\n#     max_length=15#, save_path=\"attention_heatmap\"\n# )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generate_and_visualize_attention(\n    model=model, \n    test_loader=test_loader, \n    tokenizer=token_to_index,  # Use the token_to_index as the tokenizer\n    index_to_word=index_to_word, \n    max_length=15,  # Set the maximum caption length\n    save_path=\"output/attention_map\"  # Optional: specify the path where images should be saved\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}