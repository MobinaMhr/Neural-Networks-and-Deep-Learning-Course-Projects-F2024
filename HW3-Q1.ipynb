{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":407317,"sourceType":"datasetVersion","datasetId":181273}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\nimport os\nimport glob\nimport random\nimport time\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import Adam\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader, random_split, Dataset\n\nimport torchvision.models as models\nfrom torchvision.models import VGG16_Weights\n\nimport torchvision.transforms.functional as F\n\nfrom torchvision.transforms.functional import to_tensor, resize, center_crop, rotate, adjust_brightness\nfrom torchvision.transforms.functional import resize","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T22:53:27.240584Z","iopub.execute_input":"2024-12-05T22:53:27.240985Z","iopub.status.idle":"2024-12-05T22:53:33.523027Z","shell.execute_reply.started":"2024-12-05T22:53:27.240949Z","shell.execute_reply":"2024-12-05T22:53:33.522025Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Constants","metadata":{}},{"cell_type":"markdown","source":"## constants","metadata":{}},{"cell_type":"code","source":"DATA_PATH = \"/kaggle/input/lgg-mri-segmentation/kaggle_3m/\"\n\nplt.style.use(\"dark_background\")\n\nIMG_SIZE = 512\nMASK_SIZE = 128\n\nEPOCHS = 150\nPATIENCE = 5 # Stop training if no improvement for 'patience' epochs\nBATCH_SIZE = 8","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T22:53:33.524871Z","iopub.execute_input":"2024-12-05T22:53:33.525364Z","iopub.status.idle":"2024-12-05T22:53:33.532073Z","shell.execute_reply.started":"2024-12-05T22:53:33.525325Z","shell.execute_reply":"2024-12-05T22:53:33.530775Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Constants","metadata":{}},{"cell_type":"markdown","source":"## Utility Functions","metadata":{}},{"cell_type":"code","source":"def read_data_pathes(path):\n    data_map = []\n    for sub_dir_path in glob.glob(path+\"*\"):\n        if not os.path.isdir(sub_dir_path):\n            print(\"This is not a dir:\", sub_dir_path)\n            continue\n        dirname = sub_dir_path.split(\"/\")[-1]\n        for filename in os.listdir(sub_dir_path):\n            image_path = sub_dir_path + \"/\" + filename\n            data_map.extend([dirname, image_path])\n        \n    df = pd.DataFrame({'dirname' : data_map[::2], 'path' : data_map[1::2]})\n    return df\n\ndf = read_data_pathes(DATA_PATH)\ndf.head(5)\n\ndef separate_and_sort_paths(df, base_len=89, end_img_len=4, end_mask_len=9):\n    df_imgs = df[~df['path'].str.contains(\"mask\")]\n    df_masks = df[df['path'].str.contains(\"mask\")]\n\n    imgs = sorted(df_imgs[\"path\"].values, key=lambda x: int(x[base_len:-end_img_len]))\n    masks = sorted(df_masks[\"path\"].values, key=lambda x: int(x[base_len:-end_mask_len]))\n\n    df = pd.DataFrame({'patient': df_imgs.dirname.values, 'image_path': imgs, 'mask_path': masks})\n\n    return imgs, masks, df\n\ndef prepare_sample_images(df, row_count, sample_pos_df, sample_neg_df):\n    sample_pos_df = sample_pos_df['image_path'].values\n    sample_neg_df = sample_neg_df['image_path'].values\n\n    sample_imgs = []\n    for neg, pos in zip(sample_neg_df, sample_pos_df):\n        neg_img = cv2.resize(cv2.imread(neg), (IMG_SIZE, IMG_SIZE))\n        pos_img = cv2.resize(cv2.imread(pos), (IMG_SIZE, IMG_SIZE))\n        sample_imgs.extend([pos_img, neg_img])\n\n    sample_pos_arr = np.vstack(sample_imgs[::2])\n    sample_neg_arr = np.vstack(sample_imgs[1::2])\n\n    return sample_pos_arr, sample_neg_arr\n\ndef upsample_dataset(initial_df):\n    pos_counts = initial_df['diagnosis'].value_counts() \n    max_count = pos_counts.max()\n\n    dfs = []\n    for label, count in pos_counts.items():\n        subset_df = initial_df[initial_df['diagnosis'] == label]\n        if count < max_count:\n            repeat_count = max_count - count\n            repeated_subset_df = subset_df.sample(repeat_count, replace=True)\n            dfs.append(repeated_subset_df)\n    \n    return pd.concat([initial_df] + dfs, ignore_index=True).sample(frac=1).reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T22:53:33.533734Z","iopub.execute_input":"2024-12-05T22:53:33.534199Z","iopub.status.idle":"2024-12-05T22:53:34.741969Z","shell.execute_reply.started":"2024-12-05T22:53:33.534141Z","shell.execute_reply":"2024-12-05T22:53:34.740741Z"}},"outputs":[{"name":"stdout","text":"This is not a dir: /kaggle/input/lgg-mri-segmentation/kaggle_3m/README.md\nThis is not a dir: /kaggle/input/lgg-mri-segmentation/kaggle_3m/data.csv\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def positiv_negativ_diagnosis(mask_path):\n    value = np.max(cv2.imread(mask_path))\n    if value > 0 : return 1\n    else: return 0\n\ndef get_sample_df(df, type, expected, count, column=None):\n    sample_df = df[df[type] == expected].sample(count)\n    if column: return sample_df[column]\n    return sample_df\n\ndef get_df_sample(df, type, expected, count):\n    return get_sample_df(df, type, expected, count).values\n\ndef get_img_sample(df, type, expected, count):\n    return get_sample_df(df, type, expected, count, column='image_path').values\n    \ndef resize(img, size):\n    return cv2.resize(cv2.imread(img), size)\n\ndef split(df, test_size):\n    df1, df2 = train_test_split(df, stratify=df.diagnosis, test_size=test_size)\n    return df1.reset_index(drop=True), df2.reset_index(drop=True) # Is this nessesary or we can del it, for cheating\n\ndef print_data_shapes(train_df, val_df, test_df):\n    print(f\"{'Dataset':<10} {'Shape':<15}\")\n    print(f\"{'-'*25}\")\n    print(f\"{'Train':<10} {train_df.shape}\")\n    print(f\"{'Validation':<10} {val_df.shape}\")\n    print(f\"{'Test':<10} {test_df.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T22:53:34.745040Z","iopub.execute_input":"2024-12-05T22:53:34.745708Z","iopub.status.idle":"2024-12-05T22:53:34.757125Z","shell.execute_reply.started":"2024-12-05T22:53:34.745665Z","shell.execute_reply":"2024-12-05T22:53:34.755231Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Plot Functions","metadata":{}},{"cell_type":"code","source":"def plot_histogram(df, ylabel='Image Count', title='Dataset Histogram'): \n    ax = df.diagnosis.value_counts().plot(kind='bar', stacked=True, figsize=(10, 6), color=selected_colors)\n\n    for i, rows in enumerate(df.diagnosis.value_counts().values):\n        ax.annotate(int(rows), \n                    xy=(i, rows-12), rotation=0, color='white', ha='center', verticalalignment='bottom', fontsize=15, fontweight='bold')\n\n    bounding_box = dict(boxstyle='round', fc=('lightblue'), ec=('black'))\n    ax.text(1.2, 2550, f\"Total {len(df)} images\", size=15, color='black', ha='center', va='center', bbox=bounding_box)\n    ax.set_xticklabels(labels, rotation=45, fontsize=12);\n    ax.set_ylabel(ylabel, fontsize=12)\n    ax.set_title(title,fontsize = 18, y=1.05)\n\ndef plot_distribution(df, ylabel='Total Images', title='Dataset Distribution'):\n    patients_by_diagnosis = df.groupby(['patient', 'diagnosis'])['diagnosis'].size().unstack().fillna(0)\n    patients_by_diagnosis.columns = [\"Positive\", \"Negative\"]\n    \n    ax = patients_by_diagnosis.plot(kind='bar',stacked=True, figsize=(18, 10), color=selected_colors, alpha=0.9)\n    ax.legend(fontsize=20);\n    ax.set_xlabel('Patients',fontsize = 20)\n    ax.set_ylabel(ylabel, fontsize = 20)\n    ax.set_title(title,fontsize = 25, y=1.005)\n    ax.set_xticklabels([])\n\ndef plot_masks(df):\n    sample_df = sample_pos_df.values\n    sample_imgs = []\n    for i, data in enumerate(sample_df):\n        img = resize(data[1], (IMG_SIZE, IMG_SIZE))\n        mask = resize(data[2], (MASK_SIZE, MASK_SIZE))\n        sample_imgs.extend([img, mask])\n    \n    sample_imgs_arr = np.hstack(np.array(sample_imgs[::2]))\n    sample_masks_arr = np.hstack(np.array(sample_imgs[1::2]))\n    \n    fig = plt.figure(figsize=(10, 10))\n    grid = ImageGrid(fig, 111, nrows_ncols=(2, 1), axes_pad=0.1)\n    \n    grid[0].imshow(sample_imgs_arr)\n    grid[0].set_title(\"Images\", fontsize=15)\n    grid[0].axis(\"off\")\n    grid[1].imshow(sample_masks_arr)\n    grid[1].set_title(\"Masks\", fontsize=15, y=0.9)\n    grid[1].axis(\"off\")\n    plt.show()\n\ndef plot_aug(data, mask=False):\n    ncols, nrows = len(data), 1\n    size = MASK_SIZE if mask else IMG_SIZE\n    plt.figure(figsize=(ncols * (size / 100), nrows * (size / 100)))\n    plt.subplots_adjust(wspace=0., hspace=0.)\n    idx = 0\n    for img in data:\n        if mask:\n            img = img.numpy().astype(np.float32)[0, :, :]\n        else:\n            img = img.numpy().transpose(1, 2, 0)\n\n        plt.subplot(nrows, ncols, idx + 1)\n        plt.imshow(img)\n        plt.axis('off')\n        idx += 1\n\n    return plt.show()\n\ndef plot_metrics(train_losses, val_losses, train_dice_scores, val_dice_scores):\n    epochs = range(1, len(train_losses) + 1)\n\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, train_losses, 'r', label='Train Loss')\n    plt.plot(epochs, val_losses, 'b', label='Val Loss')\n    plt.title('Loss')\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, train_dice_scores, 'r', label='Train Dice')\n    plt.plot(epochs, val_dice_scores, 'b', label='Val Dice')\n    plt.title('Dice Coefficient')\n    plt.legend()\n\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T22:53:34.759155Z","iopub.execute_input":"2024-12-05T22:53:34.759727Z","iopub.status.idle":"2024-12-05T22:53:34.782339Z","shell.execute_reply.started":"2024-12-05T22:53:34.759687Z","shell.execute_reply":"2024-12-05T22:53:34.780739Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Metric Functions","metadata":{}},{"cell_type":"code","source":"def dice_coefficient(output, target, smooth=1e-6):\n    output = (output > 0.5).float()\n    intersection = (output * target).sum()\n    return (2.0 * intersection + smooth) / (output.sum() + target.sum() + smooth)\n# # def dice_coef_metric(inputs, target):\n#     intersection = 2.0 * (target * output).sum()\n#     union = target.sum() + output.sum()\n#     if target.sum() == 0 and output.sum() == 0:\n#         return 1.0\n\n#     return intersection / union\n\ndef iou_score(output, target, smooth=1e-6):\n    output = (output > 0.5).float()\n    intersection = (output * target).sum()\n    union = output.sum() + target.sum() - intersection\n    return (intersection + smooth) / (union + smooth)\n\ndef dice_loss(output, target, smooth=1e-6):\n    return 1 - dice_coefficient(output, target)\n    # intersection = (output * target).sum()\n    # return 1 - (2.0 * intersection + smooth) / (output.sum() + target.sum() + smooth)\n\n    # smooth = 1.0\n    # intersection = 2.0 * ((target * output).sum()) + smooth\n    # union = target.sum() + output.sum() + smooth\n\n    # return 1 - (intersection / union)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T22:53:34.784475Z","iopub.execute_input":"2024-12-05T22:53:34.786034Z","iopub.status.idle":"2024-12-05T22:53:34.803436Z","shell.execute_reply.started":"2024-12-05T22:53:34.785972Z","shell.execute_reply":"2024-12-05T22:53:34.801949Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Constants","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = read_data_pathes(DATA_PATH)\ndf.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T22:53:34.805323Z","iopub.execute_input":"2024-12-05T22:53:34.805913Z","iopub.status.idle":"2024-12-05T22:53:34.918919Z","shell.execute_reply.started":"2024-12-05T22:53:34.805859Z","shell.execute_reply":"2024-12-05T22:53:34.917514Z"}},"outputs":[{"name":"stdout","text":"This is not a dir: /kaggle/input/lgg-mri-segmentation/kaggle_3m/README.md\nThis is not a dir: /kaggle/input/lgg-mri-segmentation/kaggle_3m/data.csv\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                 dirname                                               path\n0  TCGA_DU_7010_19860307  /kaggle/input/lgg-mri-segmentation/kaggle_3m/T...\n1  TCGA_DU_7010_19860307  /kaggle/input/lgg-mri-segmentation/kaggle_3m/T...\n2  TCGA_DU_7010_19860307  /kaggle/input/lgg-mri-segmentation/kaggle_3m/T...\n3  TCGA_DU_7010_19860307  /kaggle/input/lgg-mri-segmentation/kaggle_3m/T...\n4  TCGA_DU_7010_19860307  /kaggle/input/lgg-mri-segmentation/kaggle_3m/T...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dirname</th>\n      <th>path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TCGA_DU_7010_19860307</td>\n      <td>/kaggle/input/lgg-mri-segmentation/kaggle_3m/T...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TCGA_DU_7010_19860307</td>\n      <td>/kaggle/input/lgg-mri-segmentation/kaggle_3m/T...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TCGA_DU_7010_19860307</td>\n      <td>/kaggle/input/lgg-mri-segmentation/kaggle_3m/T...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TCGA_DU_7010_19860307</td>\n      <td>/kaggle/input/lgg-mri-segmentation/kaggle_3m/T...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>TCGA_DU_7010_19860307</td>\n      <td>/kaggle/input/lgg-mri-segmentation/kaggle_3m/T...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"DATA_PATH = \"/kaggle/input/lgg-mri-segmentation/kaggle_3m/\"\n\nplt.style.use(\"dark_background\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T22:53:34.921005Z","iopub.execute_input":"2024-12-05T22:53:34.921534Z","iopub.status.idle":"2024-12-05T22:53:34.928005Z","shell.execute_reply.started":"2024-12-05T22:53:34.921481Z","shell.execute_reply":"2024-12-05T22:53:34.926582Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"imgs, masks, df = separate_and_sort_paths(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T22:53:34.929956Z","iopub.execute_input":"2024-12-05T22:53:34.930391Z","iopub.status.idle":"2024-12-05T22:53:34.960163Z","shell.execute_reply.started":"2024-12-05T22:53:34.930346Z","shell.execute_reply":"2024-12-05T22:53:34.958931Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"df['diagnosis'] = df[\"mask_path\"].apply(lambda m: positiv_negativ_diagnosis(m))\n# df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T22:53:34.964129Z","iopub.execute_input":"2024-12-05T22:53:34.965251Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"positive_color = 'lightseagreen' \nnegative_color = 'violet'\nlabels = ['Positive', 'Negative']\nselected_colors = [positive_color, negative_color]\ncolors = ['mediumvioletred', 'springgreen']\nsample_pos_df = get_sample_df(df, 'diagnosis', expected=1, count=3)\nsample_neg_df = get_sample_df(df, 'diagnosis', expected=0, count=3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_histogram(df)\nplot_distribution(df)\n# plot_masks(df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# sample_yes_arr, sample_no_arr = prepare_sample_images(df, row_count=3, sample_pos_df, sample_neg_df)\n# # Plot\n# fig = plt.figure(figsize=(10, 10))\n# grid = ImageGrid(fig, 111, nrows_ncols=(1, 4), axes_pad=0.1)\n\n# grid[0].imshow(sample_yes_arr)\n# grid[0].set_title(\"Positive\")\n# grid[0].axis(\"off\")\n\n# grid[1].imshow(sample_no_arr)\n# grid[1].set_title(\"Negative\")\n# grid[1].axis(\"off\")\n\n# grid[2].imshow(sample_yes_arr[:, :, 0], cmap=\"hot\")\n# grid[2].set_title(\"Positive\")\n# grid[2].axis(\"off\")\n\n# grid[3].imshow(sample_no_arr[:, :, 0], cmap=\"hot\")\n# grid[3].set_title(\"Negative\")\n# grid[3].axis(\"off\")\n\n# plt.figtext(0.36, 0.90, \"Original\", va=\"center\", ha=\"center\", size=15)\n# plt.figtext(0.66, 0.90, \"With hot colormap\", va=\"center\", ha=\"center\", size=15)\n# plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = upsample_dataset(df)\nplot_histogram(df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df, temp_df = split(df, test_size=0.2)\ntest_df, val_df = split(temp_df, test_size=0.5)\n\nprint_data_shapes(train_df, val_df, test_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"class AugmentPair:\n    def __init__(self, rotation_range=15, scale_range=0.2, brightness_range=(0.9, 1.1)):\n        self.rotation_range = rotation_range\n        self.scale_range = scale_range\n        self.brightness_range = brightness_range\n\n    def rotate(self, input, angle):\n        return rotate(input, angle)\n\n    def scale(self, data, factor):\n        width, height = data.size\n        data = resize(data, (int(height * factor), int(width * factor)))\n        return center_crop(data, (height, width))\n    \n    def adjust_brightness(self, input, factor):\n        return adjust_brightness(input, factor)        \n        \n    def __call__(self, pair):\n        image, mask = pair\n\n        angle = np.random.uniform(-self.rotation_range, self.rotation_range)\n        image = self.rotate(image, angle)\n        mask = self.rotate(mask, angle)\n\n        factor = np.random.uniform(1 - self.scale_range, 1 + self.scale_range)\n        image = self.scale(image, factor)\n        mask = self.scale(mask, factor)\n\n        image = self.adjust_brightness(image, factor=np.random.uniform(*self.brightness_range))\n\n        return image, mask\n\nclass MRIDataset(Dataset):\n    def __init__(self, df, transformer=None):\n        self.df = df\n        self.transformer = transformer\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        # Load image and mask with OpenCV\n        image = cv2.imread(self.df.iloc[idx, 1])\n        mask = cv2.imread(self.df.iloc[idx, 2], 0)  # 0 for grayscale\n\n        # Convert to PIL Image\n        image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n        mask = Image.fromarray(mask)\n\n        # Apply transformations if provided\n        real_image = image\n        real_mask = mask\n        if self.transformer:\n            image, mask = self.transformer((image, mask))\n\n        # Convert to tensors\n        real_image = to_tensor(real_image)\n        real_mask = torch.tensor(np.array(mask) / 255.0, dtype=torch.float32).unsqueeze(0)\n        image = to_tensor(image)\n        mask = torch.tensor(np.array(mask) / 255.0, dtype=torch.float32).unsqueeze(0)\n\n        return image, resize(mask, MASK_SIZE)\n\n\ntrain_dataset = MRIDataset(train_df, transformer=AugmentPair())\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, num_workers=4, shuffle=True)\n\nval_dataset = MRIDataset(val_df, transformer=None)\nval_dataloader = DataLoader(val_dataset, batch_size=32, num_workers=4, shuffle=False)\n\ntest_dataset = MRIDataset(test_df, transformer=None)\ntest_dataloader = DataLoader(test_dataset, batch_size=32, num_workers=4, shuffle=False)\n\nimages, masks = next(iter(train_dataloader))\nprint(images.shape, masks.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# combined_df = pd.concat([sample_pos_df, sample_neg_df], ignore_index=True)\n# shuffled_df = combined_df.sample(frac=1).reset_index(drop=True)\n# dataloader_ = DataLoader(MRIDataset(shuffled_df, transformer=AugmentPair()), \n#                          batch_size=32, num_workers=4, shuffle=True)\n\n# images_, masks_, real_images_, real_masks_ = next(iter(dataloader_))\n# plot_aug(images_)\n# plot_aug(masks_)\n# plot_aug(real_images_)\n# plot_aug(real_masks_)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class VGGEncoder(nn.Module):\n    def __init__(self, pretrained=True):\n        super(VGGEncoder, self).__init__()\n        weights = VGG16_Weights.IMAGENET1K_V1 if pretrained else None\n        vgg = models.vgg16(weights=weights)\n        features = list(vgg.features.children())\n        # Extract blocks for skip connections\n        self.block1 = nn.Sequential(*features[:5])   # block1_pool\n        self.block2 = nn.Sequential(*features[5:10]) # block2_pool\n        self.block3 = nn.Sequential(*features[10:17])# block3_pool\n        self.block4 = nn.Sequential(*features[17:24])# block4_pool\n        self.block5 = nn.Sequential(*features[24:31])# block5_pool\n\n    def forward(self, x):\n        skip_connections = []\n        x = self.block1(x)\n        # print('blck1:', x.shape)\n        skip_connections.append(x)\n        x = self.block2(x)\n        # print('blck2:', x.shape)\n        skip_connections.append(x)\n        x = self.block3(x)\n        # print('blck3:', x.shape)\n        skip_connections.append(x)\n        x = self.block4(x)\n        # print('blck4:', x.shape)\n        skip_connections.append(x)\n        x = self.block5(x)\n        # print('blck5:', x.shape)\n        # print()\n        # print('VGG output shape:', x.shape)\n        return x, skip_connections[::-1]  # Reverse for decoding\n        \nclass DecoderBlock(nn.Module):\n    def __init__(self, in_channels, skip_channels, out_channels):\n        super(DecoderBlock, self).__init__()\n        self.upsample = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n        self.conv1 = nn.Conv2d(out_channels + skip_channels, out_channels, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x, skip):\n        x = self.upsample(x)  # Upsample\n        x = torch.cat([x, skip], dim=1)  # Concatenate with skip connection\n        x = self.relu(self.conv1(x))  # First convolution\n        x = self.relu(self.conv2(x))  # Second convolution\n        return x\n\nclass UNetDecoder(nn.Module):\n    def __init__(self, num_classes=1):\n        super(UNetDecoder, self).__init__()\n        self.decoder4 = DecoderBlock(512, 512, 256)  # Block 5 to Block 4\n        self.decoder3 = DecoderBlock(256, 256, 128)  # Block 4 to Block 3\n        self.decoder2 = DecoderBlock(128, 128, 64)   # Block 3 to Block 2\n        self.decoder1 = DecoderBlock(64, 64, 32)     # Block 2 to Block 1\n\n        # Final output layer\n        self.final_conv = nn.Conv2d(32, num_classes, kernel_size=1)\n\n    def forward(self, x, skips):\n        x = self.decoder4(x, skips[0])  # Skip from Block 4\n        x = self.decoder3(x, skips[1])  # Skip from Block 3\n        x = self.decoder2(x, skips[2])  # Skip from Block 2\n        x = self.decoder1(x, skips[3])  # Skip from Block 1\n        x = self.final_conv(x)          # Output layer\n        return x\n\nclass UNet(nn.Module):\n    def __init__(self, num_classes=1):\n        super(UNet, self).__init__()\n        self.encoder = VGGEncoder()\n        self.decoder = UNetDecoder()\n\n    def forward(self, x):\n        x, skips = self.encoder(x)  # Encoder forward pass\n        x = self.decoder(x, skips)  # Decoder forward pass\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_one_epoch(model, dataloader, criterion, optimizer, device):\n    model.train()\n    total_loss, total_dice, total_iou = 0, 0, 0\n\n    for images, masks in tqdm(dataloader, desc=\"Training\", leave=False):    \n        images, masks = images.to(device), masks.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = dice_loss(outputs, masks) + criterion(outputs, masks)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        total_dice += dice_coefficient(outputs, masks).item()\n        total_iou += iou_score(outputs, masks).item()\n\n    return (total_loss / len(dataloader)), (total_dice / len(dataloader)), (total_iou / len(dataloader))\n\ndef validate_one_epoch(model, dataloader, criterion, device):\n    model.eval()\n    total_loss, total_dice, total_iou = 0, 0, 0\n\n    with torch.no_grad():\n        for images, masks in tqdm(dataloader, desc=\"Validating\", leave=False):\n            images, masks = images.to(device), masks.to(device)\n            outputs = model(images)\n            loss = dice_loss(outputs, masks) + criterion(outputs, masks)\n            total_loss += loss.item()\n            total_dice += dice_coefficient(outputs, masks).item()\n            total_iou += iou_score(outputs, masks).item()\n\n    return total_loss / len(dataloader), total_dice / len(dataloader), total_iou / len(dataloader)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test_model(model, dataloader, device):\n    model.eval()\n    total_dice, total_iou = 0, 0\n    samples = []\n\n    with torch.no_grad():\n        for images, masks in dataloader:\n            images, masks = images.to(device), masks.to(device)\n            outputs = model(images)\n\n            total_dice += dice_coefficient(outputs, masks).item()\n            total_iou += iou_score(outputs, masks).item()\n\n            # Store sample predictions for visualization\n            samples.append((images.cpu(), masks.cpu(), outputs.cpu()))\n\n    avg_dice = total_dice / len(dataloader)\n    avg_iou = total_iou / len(dataloader)\n    \n    return avg_dice, avg_iou, samples\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = UNet()\nprint(model)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = Adam(model.parameters(), lr=3e-4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"early_stop_counter = 0\nbest_val_loss = np.inf\n\ntrain_losses, val_losses = [], []\ntrain_dice_scores, val_dice_scores = [], []\n\nfor epoch in range(EPOCHS):\n    train_loss, train_dice, train_iou = train_one_epoch(model, train_dataloader, criterion, optimizer, device)\n    val_loss, val_dice, val_iou = validate_one_epoch(model, val_dataloader, criterion, device)\n\n    train_losses.append(train_loss)\n    val_losses.append(val_loss)\n    train_dice_scores.append(train_dice)\n    val_dice_scores.append(val_dice)\n\n    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n    print(f\"Train Loss: {train_loss:.4f}, Train Dice: {train_dice:.4f}, Train IoU: {train_iou:.4f}\")\n    print(f\"Val Loss: {val_loss:.4f}, Val Dice: {val_dice:.4f}, Val IoU: {val_iou:.4f}\")\n\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        early_stop_counter = 0 \n        torch.save(model.state_dict(), \"best_model.pth\")\n    else:\n        early_stop_counter += 1\n        if early_stop_counter >= PATIENCE:\n            print(f\"Early stopping triggered at epoch {epoch+1}\")\n            break\n\nprint(\"Training complete.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_metrics(train_losses, val_losses, train_dice_scores, val_dice_scores)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"state_dict = torch.load(\"best_model.pth\", map_location=device)\nmodel.load_state_dict(state_dict)\nmodel.to(device) \n\ntest_loss, test_dice, test_iou = validate_one_epoch(model, test_dataloader, criterion, device)\nprint(f\"Test Loss: {test_loss:.4f}, Test Dice: {test_dice:.4f}, Test IoU: {test_iou:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # image\n# test_sample = test_df[test_df[\"diagnosis\"] == 1].sample(1).values[0]\n# image = cv2.resize(cv2.imread(test_sample[1]), (128, 128))\n\n# #mask\n# mask = cv2.resize(cv2.imread(test_sample[2]), (128, 128))\n\n# pred = model.forward(image)\n\n\n\n# # pred\n# pred = torch.tensor(image.astype(np.float32) / 255.).unsqueeze(0).permute(0,3,1,2)\n# pred = model(pred.to(device))\n# pred = validate_one_epoch(model, test_dataloader, criterion, device)\n# pred = pred.detach().cpu().numpy()[0,0,:,:]\n\n# # # pred with tshd\n# # pred_t = np.copy(pred)\n# # pred_t[np.nonzero(pred_t < 0.3)] = 0.0\n# # pred_t[np.nonzero(pred_t >= 0.3)] = 255.#1.0\n# # pred_t = pred_t.astype(\"uint8\")\n\n# # plot\n# fig, ax = plt.subplots(nrows=2,  ncols=2, figsize=(10, 10))\n\n# ax[0, 0].imshow(image)\n# ax[0, 0].set_title(\"image\")\n# ax[0, 1].imshow(mask)\n# ax[0, 1].set_title(\"mask\")\n# ax[1, 0].imshow(pred)\n# ax[1, 0].set_title(\"prediction\")\n# ax[1, 1].imshow(pred_t)\n# ax[1, 1].set_title(\"prediction with threshold\")\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}