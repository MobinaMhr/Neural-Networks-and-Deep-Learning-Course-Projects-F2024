{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport re\nimport cv2\nimport json\nimport random\nimport numpy as np\nfrom tqdm import tqdm\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom collections import defaultdict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision.models as models\nfrom torchvision import models, transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.models.efficientnet import EfficientNet_B0_Weights\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# //TODO::WTF::\n\nIMAGES_DIR = \"/kaggle/input/flickr8k/Images\"\nCAPTIONS_FILE = \"/kaggle/input/flickr8k/captions.txt\"\n\nBATCH_SIZE = 64\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_captions(captions_file):\n    captions_dict = {}\n    with open(captions_file, 'r') as file:\n        for line in file:\n            tokens = line.strip().split(\",\") \n            if len(tokens) == 2:\n                image_id, caption = tokens\n                if image_id not in captions_dict:\n                    captions_dict[image_id] = []\n                captions_dict[image_id].append(caption)\n    return captions_dict\n\ndef display_images_with_captions(images_dir, captions_dict, num_images=3, image_size=(224, 224)):\n    images = list(captions_dict.keys())\n\n    if len(images) < num_images:\n        raise ValueError(f\"Requested {num_images} images, but only {len(images)} available.\")\n    \n    selected_images = random.sample(images, num_images)\n    plt.figure(figsize=(10 * num_images, 10))\n    \n    for i, image in enumerate(selected_images):\n        img_path = os.path.join(images_dir, image)\n        img = Image.open(img_path)\n        \n        plt.subplot(num_images, 1, i + 1)\n        plt.imshow(img)\n        plt.axis('off')\n        \n        captions = \"\\n\".join(captions_dict[image]) \n        plt.title(captions, fontsize=10, loc='center', wrap=True)\n    \n    plt.tight_layout()\n    plt.show()\n\ndef plot_training_losses(train_losses, val_losses):\n    plt.figure(figsize=(10, 6))\n    plt.plot(range(1, len(train_losses) + 1), train_losses, marker='o', label='Training Loss')\n    plt.plot(range(1, len(val_losses) + 1), val_losses, marker='x', label='Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.title('Training and Validation Loss Over Epochs')\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n\ndef visualize_results(model, dataloader, tokenizer, num_images=5):\n    for images, _ in dataloader:\n        images = images[:num_images].to(device)\n        captions = [generate_caption(model, image, tokenizer) for image in images]\n\n        plt.figure(figsize=(5, num_images * 5))\n\n        for i in range(num_images):\n            ax = plt.subplot(num_images, 1, i + 1)\n            ax.imshow(images[i].permute(1, 2, 0).cpu().numpy())\n            ax.axis(\"off\")  # Remove axes\n            ax.set_title(captions[i], fontsize=12, wrap=True)\n\n        plt.tight_layout()\n        plt.show()\n        break\n\n# def display_random_images_with_captions(images_dir, captions_dict, num_images=5):\n#     selected_images = random.sample(list(captions_dict.keys()), num_images)\n#     plt.figure(figsize=(15, 10))\n\n#     for i, image_id in enumerate(selected_images):\n#         img_path = f\"{images_dir}/{image_id}\"\n#         img = Image.open(img_path)\n\n#         plt.subplot(num_images, 1, i + 1)\n#         # plt.subplot(1, num_images, i + 1)\n#         plt.imshow(img)\n#         plt.axis(\"off\")\n#         plt.title(captions_dict[image_id], fontsize=10)\n    \n#     plt.tight_layout()\n#     plt.show()\n        \ndef plot_caption_length_variation(captions_dict):\n    lengths = []\n    for captions in captions_dict.values():\n        lengths.extend([len(caption.split()) for caption in captions])\n    \n    plt.figure(figsize=(15, 10))\n    plt.scatter(range(len(lengths)), lengths, alpha=0.5)\n    plt.title(\"Variation in Caption Lengths\", fontsize=14)\n    plt.xlabel(\"Caption Index\", fontsize=12)\n    plt.ylabel(\"Caption Length (Number of Words)\", fontsize=12)\n    plt.grid(True, alpha=0.3)\n    plt.show()\n\ndef plot_word_frequency_histogram(captions_dict, top_n=20):\n    word_counter = Counter()\n    for captions in captions_dict.values():\n        for caption in captions:\n            preprocessed_caption = preprocess_text(caption)\n            word_counter.update(preprocessed_caption.split())\n    \n    most_common_words = word_counter.most_common(top_n)\n    words, counts = zip(*most_common_words)\n\n    plt.figure(figsize=(12, 6))\n    plt.bar(words, counts, color='skyblue')\n    plt.title(f\"Top {top_n} Most Frequent Words\", fontsize=14)\n    plt.xlabel(\"Words\", fontsize=12)\n    plt.ylabel(\"Frequency\", fontsize=12)\n    plt.xticks(rotation=45, fontsize=10)\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def split_dataset(captions_dict, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, random_seed=42):\n    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \"Ratios must sum to 1.\"\n\n    flattened_captions = []\n    for image_id, captions in captions_dict.items():\n        for caption in captions:\n            flattened_captions.append((image_id, caption))\n\n    random.seed(random_seed)\n    random.shuffle(flattened_captions)\n\n    total_samples = len(flattened_captions)\n    train_end = int(total_samples * train_ratio)\n    val_end = train_end + int(total_samples * val_ratio)\n\n    train_samples = flattened_captions[:train_end]\n    val_samples = flattened_captions[train_end:val_end]\n    test_samples = flattened_captions[val_end:]\n\n    train_captions = {image_id: [] for image_id, _ in train_samples}\n    for image_id, caption in train_samples:\n        train_captions[image_id].append(caption)\n\n    val_captions = {image_id: [] for image_id, _ in val_samples}\n    for image_id, caption in val_samples:\n        val_captions[image_id].append(caption)\n\n    test_captions = {image_id: [] for image_id, _ in test_samples}\n    for image_id, caption in test_samples:\n        test_captions[image_id].append(caption)\n\n    return train_captions, val_captions, test_captions\n\ndef save_captions_to_json(captions, filepath):\n    with open(filepath, 'w') as file:\n        json.dump(captions, file, ensure_ascii=False, indent=4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"captions_dict = load_captions(CAPTIONS_FILE)\ndisplay_images_with_captions(IMAGES_DIR, captions_dict)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_text(text):\n    text = text.lower()\n    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation\n    text = re.sub(r\"\\d+\", \"\", text)      # Remove numbers\n    return text.strip()\n\ndef build_tokenizer(captions_dict):\n    special_tokens = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n    word_to_index = defaultdict(lambda: special_tokens[\"<unk>\"], special_tokens.copy())\n    \n    # Tokenize captions and build vocabulary\n    vocab = set()\n    for captions in captions_dict.values():\n        for caption in captions:\n            preprocessed_caption = preprocess_text(caption)\n            vocab.update(preprocessed_caption.split())\n    \n    # Add vocabulary words to the tokenizer\n    for idx, word in enumerate(sorted(vocab), start=len(special_tokens)):\n        word_to_index[word] = idx\n    \n    return dict(word_to_index)\n\ndef save_tokenizer_to_json(tokenizer, filepath):\n    with open(filepath, 'w') as file:\n        json.dump(tokenizer, file, ensure_ascii=False, indent=4)\n\ndef caption_tokenizer(caption, word_to_index, max_length=50): # //TODO::WTF::\n    tokens = [\"<sos>\"] + caption.lower().split() + [\"<eos>\"]\n    token_ids = [word_to_index.get(word, word_to_index[\"<unk>\"]) for word in tokens]\n    token_ids = token_ids[:max_length]\n    token_ids += [word_to_index[\"<pad>\"]] * (max_length - len(token_ids))\n    return torch.tensor(token_ids)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"word_to_index_tokenizer = build_tokenizer(captions_dict)\n\nTOKENIZER_FILE = \"tokenizer.json\"\nsave_tokenizer_to_json(word_to_index_tokenizer, TOKENIZER_FILE)\n\nprint(f\"Word to Index Tokenizer saved to {TOKENIZER_FILE}. Total tokens: {len(word_to_index_tokenizer)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_captions, val_captions, test_captions = split_dataset(captions_dict)\n\nsave_captions_to_json(train_captions, \"train_captions.json\")\nsave_captions_to_json(val_captions, \"val_captions.json\")\nsave_captions_to_json(test_captions, \"test_captions.json\")\n\nprint(\"Dataset split complete:\")\nprint(f\"Training: {len(train_captions)} images\")\nprint(f\"Validation: {len(val_captions)} images\")\nprint(f\"Test: {len(test_captions)} images\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# display_random_images_with_captions(IMAGES_DIR, captions_dict, num_images=5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_caption_length_variation(captions_dict)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_word_frequency_histogram(captions_dict, top_n=20)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FlickrDataset(Dataset):\n    def __init__(self, captions_file, images_dir, transform=None, tokenizer=None, max_seq_length=50):\n        with open(captions_file, 'r') as file:\n            self.captions_dict = json.load(file)\n\n        self.images_dir = images_dir\n        self.transform = transform\n        self.tokenizer = tokenizer\n        self.max_seq_length = max_seq_length\n\n        self.image_caption_pairs = []\n        for image_id, captions in self.captions_dict.items():\n            image_path = os.path.join(images_dir, image_id)\n            if os.path.exists(image_path):\n                for caption in captions:\n                    self.image_caption_pairs.append((image_id, caption))\n        \n        random.shuffle(self.image_caption_pairs)\n    \n    def __len__(self):\n        return len(self.image_caption_pairs)\n\n    def __getitem__(self, idx):\n        image_id, caption = self.image_caption_pairs[idx]\n        image_path = os.path.join(self.images_dir, image_id)\n\n        image = Image.open(image_path).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n        \n        if self.tokenizer:\n            caption = self.tokenizer(caption)\n\n        return image, caption\n\ndef create_dataloaders(images_dir, train_captions_file, val_captions_file, test_captions_file, batch_size, tokenizer):\n    transform_train = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(p=0.25),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n        transforms.RandomRotation(degrees=15),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n\n    val_transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n    ])\n\n    train_dataset = FlickrDataset(train_captions_file, images_dir, transform=transform_train, tokenizer=tokenizer)\n    val_dataset = FlickrDataset(val_captions_file, images_dir, transform=val_transform, tokenizer=tokenizer)\n    test_dataset = FlickrDataset(test_captions_file, images_dir, transform=val_transform, tokenizer=tokenizer)\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n\n    return train_loader, val_loader, test_loader","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(TOKENIZER_FILE, \"r\") as file:\n    word_to_index = json.load(file)\n\ntrain_loader, val_loader, test_loader = create_dataloaders(\n    IMAGES_DIR,\n    \"train_captions.json\",\n    \"val_captions.json\",\n    \"test_captions.json\",\n    BATCH_SIZE,\n    lambda caption: caption_tokenizer(caption, word_to_index)\n)\n\nprint(f\"Training batches: {len(train_loader)}\")\nprint(f\"Validation batches: {len(val_loader)}\")\nprint(f\"Testing batches: {len(test_loader)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"PAD_IDX = word_to_index_tokenizer[\"<pad>\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class EncoderCNN(nn.Module):\n    def __init__(self, encoded_image_size=256):\n        super(EncoderCNN, self).__init__()\n        self.efficient_net = models.efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n        self.efficient_net.classifier = nn.Identity()\n        self.features = self.efficient_net.features\n        self.fc = nn.Linear(1280, encoded_image_size)\n\n    def forward(self, images):\n        features = self.features(images)  # (batch_size, 1280, H, W)\n        batch_size, channels, height, width = features.size()\n        features = features.permute(0, 2, 3, 1).reshape(batch_size, -1, channels)  # (batch_size, H*W, channels)\n        features = self.fc(features)  # (batch_size, H*W, encoded_image_size)\n        return features\n\n\nclass Attention(nn.Module):\n    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n        super(Attention, self).__init__()\n        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # Encoder attention\n        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # Decoder attention\n        self.full_att = nn.Linear(attention_dim, 1)  # Combine both attentions\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1)  # Normalize weights\n\n    def forward(self, encoder_out, decoder_hidden):\n        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim)\n        att2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n        alpha = self.softmax(att)  # Attention weights\n        context = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # Weighted sum of encoder outputs\n        return context, alpha\n\nclass DecoderRNN(nn.Module):\n    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=512, dropout=0.5, max_length=50):\n        super(DecoderRNN, self).__init__()\n        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # Attention network\n        print(f\"--Init--:: vocab_size:{vocab_size}, embed_dim:{embed_dim}\")\n        self.embedding = nn.Embedding(vocab_size, embed_dim)  # Embedding layer\n        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # LSTM cell\n        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # Initialize hidden state\n        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # Initialize cell state\n        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # Gate for context vector\n        self.sigmoid = nn.Sigmoid()\n        self.fc = nn.Linear(decoder_dim, vocab_size)  # Final output layer\n        self.dropout = nn.Dropout(p=dropout)\n        self.max_length = max_length  # Maximum caption length\n\n    def init_hidden_state(self, encoder_out):\n        mean_encoder_out = encoder_out.mean(dim=1)\n        h = self.init_h(mean_encoder_out)  # Hidden state\n        c = self.init_c(mean_encoder_out)  # Cell state\n        return h, c\n\n    def forward(self, encoder_out, captions):\n        batch_size = encoder_out.size(0)\n        vocab_size = self.fc.out_features\n        encoder_out = encoder_out.view(batch_size, -1, encoder_out.size(-1)) # Flatten encoder output\n        num_pixels = encoder_out.size(1)\n        h, c = self.init_hidden_state(encoder_out)\n        print(f\"Forward:: captions:{captions.shape}\")\n        embeddings = self.embedding(captions)\n        print('here2')        \n        predictions = torch.zeros(batch_size, self.max_length, vocab_size).to(encoder_out.device)\n        alphas = torch.zeros(batch_size, self.max_length, num_pixels).to(encoder_out.device)\n\n        for t in range(self.max_length):\n            context, alpha = self.attention(encoder_out, h)\n            gate = self.sigmoid(self.f_beta(h))  # Gated context vector\n            context = gate * context\n            h, c = self.decode_step(\n                torch.cat([embeddings[:, t, :], context], dim=1),\n                (h, c)\n            )\n            preds = self.fc(self.dropout(h))  # Compute predictions\n            predictions[:, t, :] = preds\n            alphas[:, t, :] = alpha\n\n        return predictions, alphas\n\nclass ImageCaptioningModel(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(ImageCaptioningModel, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, images, captions):\n        features = self.encoder(images)  # Extract image features\n        outputs, alphas = self.decoder(features, captions)  # Generate captions\n        return outputs, alphas\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def masked_cross_entropy_loss(outputs, targets):\n    loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n    return loss_fn(outputs.permute(0, 2, 1), targets)  # Permute for [batch_size, vocab_size, seq_len]\n\ndef save_checkpoint(model, optimizer, epoch, loss, checkpoint_path=\"checkpoint.pth\"):\n    checkpoint = {\n        \"model_state_dict\": model.state_dict(),\n        \"optimizer_state_dict\": optimizer.state_dict(),\n        \"epoch\": epoch,\n        \"loss\": loss,\n    }\n    torch.save(checkpoint, checkpoint_path)\n\ndef load_checkpoint(checkpoint_path, model, optimizer): # //TODO::WTF:: I don't know how to use till no.\n    checkpoint = torch.load(checkpoint_path)\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n    epoch = checkpoint[\"epoch\"]\n    loss = checkpoint[\"loss\"]\n    print(f\"Checkpoint loaded from epoch {epoch}\")\n    return epoch, loss\n\ndef generate_caption(model, image, tokenizer, max_length=20):\n    model.eval() \n\n    index_to_word = {index: word for word, index in tokenizer.items()}\n    start_token = tokenizer[\"<sos>\"]\n    end_token = tokenizer[\"<eos>\"]\n\n    with torch.no_grad():\n        features = model.encoder(image.unsqueeze(0).to(device))\n\n        caption = [start_token]  # Begin with <sos>\n        for _ in range(max_length):\n            caption_tensor = torch.tensor([caption], device=device)  # Shape: (1, current_seq_len)\n            outputs = model.decoder(features, caption_tensor)\n            next_word_idx = outputs[0, -1].argmax().item()\n            caption.append(next_word_idx)\n            if next_word_idx == end_token:\n                break\n\n    words = [index_to_word[idx] for idx in caption if idx != start_token and idx != end_token]\n    return \" \".join(words)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(\n    model, train_loader, val_loader, optimizer, num_epochs=50, \n    checkpoint_path=\"checkpoint.pth\", resume=False, patience=5,\n    weight_decay=1e-4\n):\n    model.train()\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=patience-2)\n\n    if resume and os.path.exists(checkpoint_path):\n        start_epoch, _ = load_checkpoint(checkpoint_path, model, optimizer)\n    else:\n        start_epoch = 0\n        \n    train_losses = []\n    val_losses = []\n    best_val_loss = float('inf')\n    patience_counter = 0 \n\n    for epoch in range(start_epoch, num_epochs):\n        model.train()\n        epoch_loss = 0\n        tepoch = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n        for batch_idx, (images, captions) in enumerate(tepoch):\n            images, captions = images.to(device), captions.to(device)\n            optimizer.zero_grad()\n\n            outputs = model(images, captions[:, :-1])  # Exclude the last word for teacher forcing            \n            outputs = outputs[:, :captions.size(1) - 1, :]  # Match target sequence length\n\n            loss = masked_cross_entropy_loss(outputs, captions[:, 1:])  # Exclude <sos> for target TODO::WTF:: what about the first character? it should be excluded too.\n            loss.backward()\n            optimizer.step()\n\n            epoch_loss += loss.item()\n\n            tepoch.set_postfix(loss=f\"{loss.item():.4f}\")\n\n        train_losses.append(epoch_loss / len(train_loader))\n        print(f\"Epoch {epoch+1}, Train Loss: {train_losses[-1]:.4f}\")\n\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for images, captions in val_loader:\n                images, captions = images.to(device), captions.to(device)\n                outputs = model(images, captions[:, :-1])\n                outputs = outputs[:, :captions.size(1) - 1, :]\n                loss = masked_cross_entropy_loss(outputs, captions[:, 1:])\n                val_loss += loss.item()\n\n        val_loss /= len(val_loader)\n        val_losses.append(val_loss)\n        print(f\"Epoch {epoch+1}, Validation Loss: {val_loss:.4f}\")\n\n        scheduler.step(val_loss)\n        # print(f\"Learning rate: {optimizer.param_groups[0]['lr']}\")\n\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0 \n            save_checkpoint(model, optimizer, epoch + 1, val_loss, checkpoint_path)\n        else:\n            patience_counter += 1\n            print(f\"No improvement in validation loss for {patience_counter} epoch(s).\")\n\n        if patience_counter >= patience:\n            print(\"Early stopping triggered.\")\n            break\n\n    return train_losses, val_losses","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vocab_size = len(word_to_index_tokenizer)  # Tokenizer vocabulary size\nembed_size = 512       # Size of image feature vector and word embeddings\nhidden_size = 512      # Size of the LSTM hidden state\nnum_layers = 1         # Number of layers in LSTM\ndropout = 0.5          # Dropout rate\nlearning_rate = 0.001  # Initial learning rate\nweight_decay = 1e-4    # Weight decay for regularization\n\nencoder = EncoderCNN(encoded_image_size=embed_size).to(device)\ndecoder = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers, dropout=dropout).to(device)\nmodel = ImageCaptioningModel(encoder, decoder).to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n# print(model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_losses, val_losses = train_model(\n    model, train_loader, val_loader, optimizer, num_epochs=50, \n    checkpoint_path=\"caption_model.pth\", patience=5\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_training_losses(train_losses, val_losses)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"visualize_results(model, test_loader, word_to_index_tokenizer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}