{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30806,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import warnings\n# warnings.filterwarnings(\"ignore\")\nimport os\nimport zipfile\nimport tensorflow as tf\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.layers import Conv2D, UpSampling2D, Concatenate, Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom PIL import Image \nfrom collections import Counter\nimport random\n\nfrom sklearn.model_selection import train_test_split\nfrom IPython.display import clear_output\n\nimport torch\nimport torchvision\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import functional as F\n\nfrom torchvision.ops import box_iou\nfrom torchvision.models.detection.faster_rcnn import FasterRCNN\nfrom torchmetrics.detection.mean_ap import MeanAveragePrecision\nfrom tqdm import tqdm  # Import tqdm for progress bar","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T10:38:30.732035Z","iopub.execute_input":"2024-12-07T10:38:30.732534Z","iopub.status.idle":"2024-12-07T10:38:48.351145Z","shell.execute_reply.started":"2024-12-07T10:38:30.732506Z","shell.execute_reply":"2024-12-07T10:38:48.350243Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Constants","metadata":{}},{"cell_type":"code","source":"url = \"https://sid.erda.dk/public/archives/ff17dc924eba88d5d01a807357d6614c/FullIJCNN2013.zip\"\nfilename = \"FullIJCNN2013.zip\"\n\ndata_dir = '/kaggle/working/FullIJCNN2013/FullIJCNN2013'\ngt_file = '/kaggle/working/FullIJCNN2013/FullIJCNN2013/gt.txt'\n\nannotations_file = \"/kaggle/working/FullIJCNN2013/FullIJCNN2013/gt.txt\"\nimages_dir       = \"/kaggle/working/FullIJCNN2013/FullIJCNN2013\"\n\nclass_number = 5\nnum_classes = 5 #4  \n\nclass_mapping = {\n    0: \"speed limit 20 (prohibitory)\",\n    1: \"speed limit 30 (prohibitory)\",\n    2: \"speed limit 50 (prohibitory)\",\n    3: \"speed limit 60 (prohibitory)\",\n    4: \"speed limit 70 (prohibitory)\",\n    5: \"speed limit 80 (prohibitory)\",\n    6: \"restriction ends 80 (other)\",\n    7: \"speed limit 100 (prohibitory)\",\n    8: \"speed limit 120 (prohibitory)\",\n    9: \"no overtaking (prohibitory)\",\n    10: \"no overtaking (trucks) (prohibitory)\",\n    11: \"priority at next intersection (danger)\",\n    12: \"priority road (other)\",\n    13: \"give way (other)\",\n    14: \"stop (other)\",\n    15: \"no traffic both ways (prohibitory)\",\n    16: \"no trucks (prohibitory)\",\n    17: \"no entry (other)\",\n    18: \"danger (danger)\",\n    19: \"bend left (danger)\",\n    20: \"bend right (danger)\",\n    21: \"bend (danger)\",\n    22: \"uneven road (danger)\",\n    23: \"slippery road (danger)\",\n    24: \"road narrows (danger)\",\n    25: \"construction (danger)\",\n    26: \"traffic signal (danger)\",\n    27: \"pedestrian crossing (danger)\",\n    28: \"school crossing (danger)\",\n    29: \"cycles crossing (danger)\",\n    30: \"snow (danger)\",\n    31: \"animals (danger)\",\n    32: \"restriction ends (other)\",\n    33: \"go right (mandatory)\",\n    34: \"go left (mandatory)\",\n    35: \"go straight (mandatory)\",\n    36: \"go right or straight (mandatory)\",\n    37: \"go left or straight (mandatory)\",\n    38: \"keep right (mandatory)\",\n    39: \"keep left (mandatory)\",\n    40: \"roundabout (mandatory)\",\n    41: \"restriction ends (overtaking) (other)\",\n    42: \"restriction ends (overtaking (trucks)) (other)\"\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T10:38:48.352530Z","iopub.execute_input":"2024-12-07T10:38:48.353028Z","iopub.status.idle":"2024-12-07T10:38:48.361094Z","shell.execute_reply.started":"2024-12-07T10:38:48.353000Z","shell.execute_reply":"2024-12-07T10:38:48.360254Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Plot Functions","metadata":{}},{"cell_type":"code","source":"def plot_annotations(image_path, annotations, image_number):\n    image = Image.open(image_path)\n    plt.imshow(image)\n    plt.axis('off')\n\n    plt.title(f\"Image: {image_number}\")\n\n    for annotation in annotations:\n        left, top, right, bottom, class_id = annotation\n        class_name = class_mapping.get(class_id, \"Unknown class\")\n        rect = plt.Rectangle((left, top), right-left, bottom-top, edgecolor='red', facecolor='none', linewidth=1)\n        plt.gca().add_patch(rect)\n        plt.text(left, top-20, class_name, color='red', fontsize=8, backgroundcolor='white', bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))\n\n    plt.show()\n\ndef plot_samples(data_dir, gt_file, sample_count):\n    annotations = load_annotations(gt_file)\n    img_files = sorted([f for f in os.listdir(data_dir) if f.endswith('.ppm')])\n\n    random.shuffle(img_files)\n    selected_files = img_files[:sample_count]\n\n    for img_file in selected_files:\n        img_path = os.path.join(data_dir, img_file)\n        image_number = img_file.split('.')[0]\n        if img_file in annotations:\n            plot_annotations(img_path, annotations[img_file], image_number)\n\ndef plot_histogram(object_sizes):\n    categories = list(object_sizes.keys())\n    counts = list(object_sizes.values())\n\n    plt.bar(categories, counts, color=['blue', 'orange', 'green'])\n    plt.xlabel('Object Size')\n    plt.ylabel('Count')\n    plt.title('Object Size Distribution')\n    plt.show()\n\ndef plot_class_histogram(data, title, xlabel, ylabel, color='purple'):\n    categories = list(data.keys())\n    counts = list(data.values())\n\n    plt.figure(figsize=(12, 8))  # Increase figure size for better spacing\n    plt.barh(categories, counts, color=color, height=0.8)  # Increase the height of bars for more spacing\n    plt.xlabel(ylabel)  # Switch xlabel and ylabel for horizontal bar chart\n    plt.ylabel(xlabel)\n    plt.title(title)\n    plt.gca().invert_yaxis()  # Invert y-axis to plot right to left\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T10:38:48.361878Z","iopub.execute_input":"2024-12-07T10:38:48.362166Z","iopub.status.idle":"2024-12-07T10:38:48.377518Z","shell.execute_reply.started":"2024-12-07T10:38:48.362141Z","shell.execute_reply":"2024-12-07T10:38:48.376725Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Utility functions","metadata":{}},{"cell_type":"code","source":"def get_class_value(class_number):\n    return class_mapping.get(class_number, \"Unknown class number\")\n\ndef load_annotations(gt_file):\n    annotations = {}\n    with open(gt_file, 'r') as file:\n        lines = file.readlines()\n        for line in lines:\n            parts = line.strip().split(';')\n            img_file = parts[0]\n            left, top, right, bottom, class_id = map(int, parts[1:])\n            if img_file not in annotations:\n                annotations[img_file] = []\n            annotations[img_file].append((left, top, right, bottom, class_id))\n    return annotations\n\ndef categorize_objects(annotations):\n    object_sizes = {'small': 0, 'medium': 0, 'large': 0}\n    for img_file, objs in annotations.items():\n        for obj in objs:\n            left, top, right, bottom, class_id = obj\n            width = right - left\n            if width < 32:\n                object_sizes['small'] += 1\n            elif 32 <= width and width < 45:\n                object_sizes['medium'] += 1\n            else:\n                object_sizes['large'] += 1\n    return object_sizes\n\ndef count_objects_by_class(annotations):\n    class_counts = Counter()\n    for img_file, objs in annotations.items():\n        for obj in objs:\n            class_id = obj[4]\n            class_name = class_mapping.get(class_id, \"Unknown class\")\n            class_counts[class_name] += 1\n    return class_counts\n\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\ndef get_dataloaders(dataset, batch_size=4):\n    train_dataset, val_dataset, test_dataset = dataset.split_dataset(val_ratio=0.1, test_ratio=0.2)\n    print(f'Size of Dataset: {len(dataset)}')\n    print(f'------------------------------')\n    print(f'Size of Train Dataset:      {len(train_dataset)}')\n    print(f'Size of Test Dataset:       {len(val_dataset)}')\n    print(f'Size of Validation Dataset: {len(test_dataset)}')\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n    val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n    test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n    return train_loader, val_loader, test_loader \n\ndef get_model(num_classes):\n    model = fasterrcnn_resnet50_fpn(weights=\"COCO_V1\")\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(\n        in_features, num_classes\n    )\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T10:38:48.379835Z","iopub.execute_input":"2024-12-07T10:38:48.380358Z","iopub.status.idle":"2024-12-07T10:38:48.394637Z","shell.execute_reply.started":"2024-12-07T10:38:48.380322Z","shell.execute_reply":"2024-12-07T10:38:48.393903Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Metric Functions","metadata":{}},{"cell_type":"code","source":"def calculate_iou(boxA, boxB):\n    # Calculate the coordinates of the intersection rectangle\n    inter_x1 = max(boxA[0], boxB[0])\n    inter_y1 = max(boxA[1], boxB[1])\n    inter_x2 = min(boxA[2], boxB[2])\n    inter_y2 = min(boxA[3], boxB[3])\n\n    # Compute the area of the intersection rectangle\n    inter_width = max(0, inter_x2 - inter_x1)\n    inter_height = max(0, inter_y2 - inter_y1)\n    inter_area = inter_width * inter_height\n\n    # Compute the area of both bounding boxes\n    boxA_area = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n    boxB_area = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n\n    # Compute the IoU\n    iou = inter_area / float(boxA_area + boxB_area - inter_area)\n\n    return iou\n\ndef precision_recall_curve(true_positives, false_positives, total_ground_truths):\n    precisions = []\n    recalls = []\n    \n    tp_cumsum = torch.cumsum(true_positives, dim=0)\n    fp_cumsum = torch.cumsum(false_positives, dim=0)\n    \n    for tp, fp in zip(tp_cumsum, fp_cumsum):\n        precision = tp / (tp + fp) if tp + fp > 0 else 0\n        recall = tp / total_ground_truths if total_ground_truths > 0 else 0\n        precisions.append(precision)\n        recalls.append(recall)\n    \n    return precisions, recalls\n\ndef compute_average_precision(precisions, recalls):\n    # Add boundary points for the curve\n    precisions = [0] + precisions + [0]\n    recalls = [0] + recalls + [1]\n    \n    # Interpolate precisions to ensure monotonicity\n    for i in range(len(precisions) - 2, -1, -1):\n        precisions[i] = max(precisions[i], precisions[i + 1])\n    \n    # Calculate AP as the area under the curve\n    ap = 0\n    for i in range(1, len(recalls)):\n        ap += (recalls[i] - recalls[i - 1]) * precisions[i]\n    \n    return ap\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T10:38:48.395443Z","iopub.execute_input":"2024-12-07T10:38:48.395697Z","iopub.status.idle":"2024-12-07T10:38:48.411060Z","shell.execute_reply.started":"2024-12-07T10:38:48.395675Z","shell.execute_reply":"2024-12-07T10:38:48.410315Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Implementation","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T10:38:48.411870Z","iopub.execute_input":"2024-12-07T10:38:48.412111Z","iopub.status.idle":"2024-12-07T10:38:48.483265Z","shell.execute_reply.started":"2024-12-07T10:38:48.412083Z","shell.execute_reply":"2024-12-07T10:38:48.482310Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"try:\n    os.system(f\"wget -O {filename} {url}\")\n    clear_output(wait=True)\n    print(f\"{filename} downloaded successfully.\")\nexcept Exception as e:\n    print(\"An error occurred while downloading the dataset:\", str(e))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T10:38:48.484439Z","iopub.execute_input":"2024-12-07T10:38:48.484801Z","iopub.status.idle":"2024-12-07T10:40:40.726528Z","shell.execute_reply.started":"2024-12-07T10:38:48.484764Z","shell.execute_reply":"2024-12-07T10:40:40.725706Z"}},"outputs":[{"name":"stdout","text":"FullIJCNN2013.zip downloaded successfully.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"if os.path.exists(filename):\n    try:\n        with zipfile.ZipFile(filename, 'r') as zip_ref:\n            zip_ref.extractall(\"FullIJCNN2013\")  \n        print(f\"{filename} extracted successfully.\")\n    except Exception as e:\n        print(\"An error occurred while extracting the dataset:\", str(e))\nelse:\n    print(f\"{filename} not found for extraction.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T10:40:40.727569Z","iopub.execute_input":"2024-12-07T10:40:40.727862Z","iopub.status.idle":"2024-12-07T10:41:01.996809Z","shell.execute_reply.started":"2024-12-07T10:40:40.727836Z","shell.execute_reply":"2024-12-07T10:41:01.995837Z"}},"outputs":[{"name":"stdout","text":"FullIJCNN2013.zip extracted successfully.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# print(f\"Class {class_number}: {get_class_value(class_number)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T10:41:01.997762Z","iopub.execute_input":"2024-12-07T10:41:01.998007Z","iopub.status.idle":"2024-12-07T10:41:02.002026Z","shell.execute_reply.started":"2024-12-07T10:41:01.997982Z","shell.execute_reply":"2024-12-07T10:41:02.001153Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# plot_samples(data_dir, gt_file, sample_count=3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T10:41:02.005131Z","iopub.execute_input":"2024-12-07T10:41:02.005912Z","iopub.status.idle":"2024-12-07T10:41:02.064142Z","shell.execute_reply.started":"2024-12-07T10:41:02.005884Z","shell.execute_reply":"2024-12-07T10:41:02.063283Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"annotations = load_annotations(gt_file)\nobject_sizes = categorize_objects(annotations)\n# plot_histogram(object_sizes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T10:41:02.064962Z","iopub.execute_input":"2024-12-07T10:41:02.065250Z","iopub.status.idle":"2024-12-07T10:41:02.078379Z","shell.execute_reply.started":"2024-12-07T10:41:02.065220Z","shell.execute_reply":"2024-12-07T10:41:02.077546Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class_counts = count_objects_by_class(annotations)\n# plot_class_histogram(class_counts, \"Class Distribution\", \"Class\", \"Count\", color='purple')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T10:41:02.079303Z","iopub.execute_input":"2024-12-07T10:41:02.079605Z","iopub.status.idle":"2024-12-07T10:41:02.086262Z","shell.execute_reply.started":"2024-12-07T10:41:02.079553Z","shell.execute_reply":"2024-12-07T10:41:02.085466Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"from torch.utils.data import Dataset, Subset\n\nclass FullIJCNN2013Dataset(Dataset):\n    def __init__(self, annotations_file, images_dir, transform=None):\n        self.annotations = self._load_annotations(annotations_file)\n        self.images_dir = images_dir\n        self.transform = transform\n        self.label_to_id = {\n            \"prohibitory\": 1,\n            \"danger\": 2,\n            \"mandatory\": 3,\n            \"other\": 4\n        }\n\n    def _load_annotations(self, annotations_file):\n        category_mapping = {\n            \"prohibitory\": list(range(0, 6)) + list(range(7, 11)) + [15, 16],\n            \"danger\": list(range(18, 32)) + [11],\n            \"mandatory\": list(range(33, 41)),\n            \"other\": [6, 12, 13, 14, 17, 32, 41, 42]\n        }\n\n        category_lookup = {}\n        for category, ids in category_mapping.items():\n            for cid in ids:\n                category_lookup[cid] = category\n\n        annotations = {}\n        with open(annotations_file, \"r\") as file:\n            for line in file:\n                parts = line.strip().split(\";\")\n                img_name = parts[0]\n                bbox = tuple(map(int, parts[1:5]))\n                class_id = int(parts[5])\n                category = category_lookup.get(class_id, \"other\")\n\n                if img_name not in annotations:\n                    annotations[img_name] = []\n                annotations[img_name].append({\"bbox\": bbox, \"label\": category})\n        return annotations\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, idx):\n        img_name = list(self.annotations.keys())[idx]\n        img_path = os.path.join(self.images_dir, img_name)\n        image = Image.open(img_path).convert(\"RGB\")\n\n        # # Convert image to a PyTorch tensor and normalize pixel values\n        # image = torch.tensor(np.array(image), dtype=torch.float32) / 255.0\n        # image = image.permute(2, 0, 1)  # Change from HWC to CHW format\n\n        objects = self.annotations[img_name]\n        bboxes = torch.tensor([obj[\"bbox\"] for obj in objects], dtype=torch.float32)\n        labels = torch.tensor(\n            [self.label_to_id[obj[\"label\"]] for obj in objects], dtype=torch.int64\n        )\n\n        target = {\n            \"boxes\": bboxes,\n            \"labels\": labels\n        }\n\n        if self.transform: image = self.transform(image)\n\n        return image, target\n\n    def split_dataset(self, val_ratio=0.1, test_ratio=0.2, random_state=123):\n        indices = list(range(len(self)))\n        temp_indices, test_indices = train_test_split(indices, test_size=test_ratio, random_state=random_state)\n        train_indices, val_indices = train_test_split(temp_indices, test_size=val_ratio, random_state=random_state)\n        \n        train_dataset = Subset(self, train_indices)\n        val_dataset   = Subset(self, val_indices)\n        test_dataset  = Subset(self, test_indices)\n        return train_dataset, val_dataset, test_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T10:41:02.087144Z","iopub.execute_input":"2024-12-07T10:41:02.087350Z","iopub.status.idle":"2024-12-07T10:41:02.099260Z","shell.execute_reply.started":"2024-12-07T10:41:02.087329Z","shell.execute_reply":"2024-12-07T10:41:02.098431Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from torchvision import transforms\nimport torchvision.transforms as T\n\ntransform = T.Compose([\n    T.ToTensor(),  # Converts PIL.Image to torch.Tensor and normalizes to [0, 1]\n])\n\ndataset = FullIJCNN2013Dataset(annotations_file, images_dir, transform=transform)\ntrain_loader, val_loader, test_loader = get_dataloaders(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T10:41:02.100188Z","iopub.execute_input":"2024-12-07T10:41:02.100443Z","iopub.status.idle":"2024-12-07T10:41:02.117332Z","shell.execute_reply.started":"2024-12-07T10:41:02.100420Z","shell.execute_reply":"2024-12-07T10:41:02.116615Z"}},"outputs":[{"name":"stdout","text":"Size of Dataset: 741\n------------------------------\nSize of Train Dataset:      532\nSize of Test Dataset:       60\nSize of Validation Dataset: 149\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"!pip install -U pycocotools\n!pip install -U faster-coco-eval\n!pip install -U torchmetrics[detection]\nclear_output(wait=True)\nos.system('clear')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T10:41:02.118324Z","iopub.execute_input":"2024-12-07T10:41:02.118565Z","iopub.status.idle":"2024-12-07T10:41:30.009487Z","shell.execute_reply.started":"2024-12-07T10:41:02.118542Z","shell.execute_reply":"2024-12-07T10:41:30.008487Z"}},"outputs":[{"name":"stdout","text":"\u001b[H\u001b[2J","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"def box_iou(boxes1, boxes2):\n    if boxes1.numel() == 0 or boxes2.numel() == 0:\n        return torch.zeros((boxes1.size(0), boxes2.size(0)), device=boxes1.device)\n\n    x1 = torch.max(boxes1[:, None, 0], boxes2[:, 0])  # Top-left x\n    y1 = torch.max(boxes1[:, None, 1], boxes2[:, 1])  # Top-left y\n    x2 = torch.min(boxes1[:, None, 2], boxes2[:, 2])  # Bottom-right x\n    y2 = torch.min(boxes1[:, None, 3], boxes2[:, 3])  # Bottom-right y\n\n    inter_width = (x2 - x1).clamp(min=0)\n    inter_height = (y2 - y1).clamp(min=0)\n    inter_area = inter_width * inter_height\n\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])  # Area of boxes1\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])  # Area of boxes2\n\n    union_area = area1[:, None] + area2 - inter_area\n    iou = inter_area / union_area\n\n    return iou\n\ndef early_stop(metric_values, patience):\n    if not metric_values: return False\n    best_metric = max(metric_values)\n    best_metric_epoch = len(metric_values) - 1 - metric_values[::-1].index(best_metric)\n    if len(metric_values) - best_metric_epoch - 1 >= patience:\n        recent_metrics = metric_values[best_metric_epoch + 1:]\n        if all(m <= best_metric for m in recent_metrics[:patience]): return True\n    return False\n\ndef compute_iou_loss(model, val_loader):\n    model.eval()\n    total_iou = 0\n    total_samples = 0\n    with torch.no_grad():\n        for images, targets in val_loader:\n            images = list(img.to(device) for img in images)\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n            \n            outputs = model(images)\n            \n            for output, target in zip(outputs, targets):\n                pred_boxes, target_boxes = output['boxes'], target['boxes']\n                if pred_boxes.numel() == 0 or target_boxes.numel() == 0:\n                    print(f\"Empty boxes found. Predicted: {pred_boxes.numel()}, Target: {target_boxes.numel()}\")\n                    continue\n                \n                ious = box_iou(pred_boxes, target_boxes)                   \n                if ious.numel() == 0: \n                    print(f\"No valid IoUs found between predicted and target boxes.\")\n                    continue\n                \n                mean_iou = ious.mean().item()\n                if torch.isnan(torch.tensor(mean_iou)):\n                    print(f\"NaN IoU found. Predicted boxes: {pred_boxes}, Target boxes: {target_boxes}\")\n                    continue\n                \n                total_iou += mean_iou\n                total_samples += 1\n    \n    if total_samples == 0:\n        print(\"No valid samples found in validation set for IoU computation.\")\n        return float('nan')  \n    \n    return total_iou / total_samples\n\ndef compute_map(model, val_loader):\n    model.eval()\n    aps = []\n\n    with torch.no_grad():\n        for images, targets in val_loader:\n            images = list(img.to(device) for img in images)\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n            outputs = model(images)\n\n            for output, target in zip(outputs, targets):\n                pred_boxes, pred_scores = output['boxes'], output['scores']\n                target_boxes = target['boxes']\n\n                if pred_boxes.numel() == 0 or target_boxes.numel() ==0:\n                    aps.append(0)\n                    continue\n\n                ious = box_iou(pred_boxes, target_boxes)\n                \n                # Assign detections to ground truths\n                max_ious, max_indices = ious.max(dim=1)\n                true_positives = (max_ious >= 0.5).float()\n                false_positives = 1 - true_positives\n                \n                # Sort predictions by confidence\n                sorted_indices = pred_scores.argsort(descending=True)\n                true_positives = true_positives[sorted_indices]\n                false_positives = false_positives[sorted_indices]\n                \n                # Precision-recall curve\n                precisions, recalls = precision_recall_curve(true_positives, false_positives, len(target_boxes))\n                \n                # Average precision for this image\n                ap = compute_average_precision(precisions, recalls)\n                aps.append(ap)\n\n    return sum(aps) / len(aps) if aps else 0\n\ndef run_stage(model, train_loader, val_loader, optimizer, num_epochs, patience, stage_name, eval_metric_fn):\n    best_metric = 0\n    metric_values = []\n    for epoch in range(num_epochs):\n        print(f\"\\n{stage_name} Training - Epoch {epoch + 1}/{num_epochs}\")\n        model.train()\n        epoch_loss = 0\n        for images, targets in tqdm(train_loader, desc=\"Batch\", leave=True, ncols=100):\n            images = list(img.to(device) for img in images)\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n            loss_dict = model(images, targets)\n            total_loss = sum(loss for loss in loss_dict.values())\n            \n            optimizer.zero_grad()\n            total_loss.backward()\n            optimizer.step()\n            \n            epoch_loss += total_loss.item()\n        \n        print(f\"Epoch {epoch + 1}/{num_epochs} - Loss: {epoch_loss:.4f}\")\n        \n        metric_value = eval_metric_fn(model, val_loader)\n        metric_values.append(metric_value)\n        print(f\"Validation Metric ({stage_name}): {metric_value:.4f}\")\n        \n        if metric_value > best_metric:\n            best_metric = metric_value\n            torch.save(model.state_dict(), f\"best_{stage_name.lower()}_model.pth\")\n            print(f\"Best model for {stage_name} saved.\")\n        \n        if early_stop(metric_values, patience):\n            print(f\"Early stopping triggered for {stage_name}.\")\n            break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T10:41:30.011265Z","iopub.execute_input":"2024-12-07T10:41:30.011734Z","iopub.status.idle":"2024-12-07T10:41:30.031767Z","shell.execute_reply.started":"2024-12-07T10:41:30.011687Z","shell.execute_reply":"2024-12-07T10:41:30.030918Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def train_faster_rcnn_staged(model, train_loader, val_loader, device, rpn_epochs, roi_epochs, \n                             fine_tune_epochs, rpn_patience=3, roi_patience=3, fine_tune_patience=3, lr=1e-3):\n    print(\"Starting staged training...\")\n    # Stage: Train ROI Head\n    for param in model.backbone.parameters():\n        param.requires_grad = False\n    for param in model.roi_heads.parameters():\n        param.requires_grad = True\n    for param in model.rpn.parameters():\n        param.requires_grad = False\n    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n    run_stage(model, train_loader, val_loader, optimizer, roi_epochs, roi_patience, \"ROI Head\", compute_iou_loss)\n    \n    # Stage: Train RPN\n    for param in model.backbone.parameters():\n        param.requires_grad = False\n    for param in model.roi_heads.parameters():\n        param.requires_grad = False\n    for param in model.rpn.parameters():\n        param.requires_grad = True\n    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n    run_stage(model, train_loader, val_loader, optimizer, rpn_epochs, rpn_patience, \"RPN\", compute_iou_loss)\n    \n    # Stage: Train ROI Head\n    for param in model.backbone.parameters():\n        param.requires_grad = False\n    for param in model.roi_heads.parameters():\n        param.requires_grad = True\n    for param in model.rpn.parameters():\n        param.requires_grad = False\n    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n    run_stage(model, train_loader, val_loader, optimizer, roi_epochs, roi_patience, \"ROI Head\", compute_map)\n    \n    # Stage: Fine-tune Entire Network\n    for param in model.parameters():\n        param.requires_grad = True\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr * 0.1)  # Lower learning rate for fine-tuning\n    run_stage(model, train_loader, val_loader, optimizer, fine_tune_epochs, fine_tune_patience, \"Fine-tuning\", compute_map)\n\n    print(\"Staged training complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T10:41:30.032937Z","iopub.execute_input":"2024-12-07T10:41:30.033183Z","iopub.status.idle":"2024-12-07T10:41:30.051008Z","shell.execute_reply.started":"2024-12-07T10:41:30.033160Z","shell.execute_reply":"2024-12-07T10:41:30.050172Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"faster_rcnn_model = get_model(num_classes).to(device)\ntrain_faster_rcnn_staged(\n    faster_rcnn_model, train_loader, val_loader, device, \n    rpn_epochs=3, roi_epochs=3, fine_tune_epochs=3, # 100\n    rpn_patience=5, roi_patience=5, fine_tune_patience=5,\n    lr=1e-3\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T10:41:30.051969Z","iopub.execute_input":"2024-12-07T10:41:30.052215Z","iopub.status.idle":"2024-12-07T10:54:25.064392Z","shell.execute_reply.started":"2024-12-07T10:41:30.052191Z","shell.execute_reply":"2024-12-07T10:54:25.063528Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n100%|██████████| 160M/160M [00:00<00:00, 186MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Starting staged training...\n\nROI Head Training - Epoch 1/3\n","output_type":"stream"},{"name":"stderr","text":"Batch: 100%|██████████████████████████████████████████████████████| 133/133 [00:44<00:00,  2.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3 - Loss: 22.0441\nEmpty boxes found. Predicted: 0, Target: 8\nEmpty boxes found. Predicted: 0, Target: 4\nValidation Metric (ROI Head): 0.2592\nBest model for ROI Head saved.\n\nROI Head Training - Epoch 2/3\n","output_type":"stream"},{"name":"stderr","text":"Batch: 100%|██████████████████████████████████████████████████████| 133/133 [00:44<00:00,  2.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/3 - Loss: 15.4163\nEmpty boxes found. Predicted: 0, Target: 8\nEmpty boxes found. Predicted: 0, Target: 8\nEmpty boxes found. Predicted: 0, Target: 4\nEmpty boxes found. Predicted: 0, Target: 16\nValidation Metric (ROI Head): 0.3785\nBest model for ROI Head saved.\n\nROI Head Training - Epoch 3/3\n","output_type":"stream"},{"name":"stderr","text":"Batch: 100%|██████████████████████████████████████████████████████| 133/133 [00:45<00:00,  2.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/3 - Loss: 13.6692\nEmpty boxes found. Predicted: 0, Target: 4\nEmpty boxes found. Predicted: 0, Target: 8\nEmpty boxes found. Predicted: 0, Target: 8\nEmpty boxes found. Predicted: 0, Target: 4\nEmpty boxes found. Predicted: 0, Target: 16\nValidation Metric (ROI Head): 0.4225\nBest model for ROI Head saved.\n\nRPN Training - Epoch 1/3\n","output_type":"stream"},{"name":"stderr","text":"Batch: 100%|██████████████████████████████████████████████████████| 133/133 [00:47<00:00,  2.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3 - Loss: 21.0725\nValidation Metric (RPN): 0.3135\nBest model for RPN saved.\n\nRPN Training - Epoch 2/3\n","output_type":"stream"},{"name":"stderr","text":"Batch: 100%|██████████████████████████████████████████████████████| 133/133 [00:48<00:00,  2.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/3 - Loss: 22.3671\nValidation Metric (RPN): 0.3417\nBest model for RPN saved.\n\nRPN Training - Epoch 3/3\n","output_type":"stream"},{"name":"stderr","text":"Batch: 100%|██████████████████████████████████████████████████████| 133/133 [00:47<00:00,  2.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/3 - Loss: 19.4293\nValidation Metric (RPN): 0.3540\nBest model for RPN saved.\n\nROI Head Training - Epoch 1/3\n","output_type":"stream"},{"name":"stderr","text":"Batch: 100%|██████████████████████████████████████████████████████| 133/133 [00:44<00:00,  3.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3 - Loss: 15.4188\nValidation Metric (ROI Head): 1.7736\nBest model for ROI Head saved.\n\nROI Head Training - Epoch 2/3\n","output_type":"stream"},{"name":"stderr","text":"Batch: 100%|██████████████████████████████████████████████████████| 133/133 [00:45<00:00,  2.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/3 - Loss: 12.1727\nValidation Metric (ROI Head): 1.4336\n\nROI Head Training - Epoch 3/3\n","output_type":"stream"},{"name":"stderr","text":"Batch: 100%|██████████████████████████████████████████████████████| 133/133 [00:46<00:00,  2.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/3 - Loss: 10.8937\nValidation Metric (ROI Head): 1.4090\n\nFine-tuning Training - Epoch 1/3\n","output_type":"stream"},{"name":"stderr","text":"Batch: 100%|██████████████████████████████████████████████████████| 133/133 [01:38<00:00,  1.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3 - Loss: 11.2502\nValidation Metric (Fine-tuning): 1.3972\nBest model for Fine-tuning saved.\n\nFine-tuning Training - Epoch 2/3\n","output_type":"stream"},{"name":"stderr","text":"Batch: 100%|██████████████████████████████████████████████████████| 133/133 [01:38<00:00,  1.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/3 - Loss: 7.7976\nValidation Metric (Fine-tuning): 1.2449\n\nFine-tuning Training - Epoch 3/3\n","output_type":"stream"},{"name":"stderr","text":"Batch: 100%|██████████████████████████████████████████████████████| 133/133 [01:38<00:00,  1.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/3 - Loss: 5.8765\nValidation Metric (Fine-tuning): 1.1342\nStaged training complete.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}